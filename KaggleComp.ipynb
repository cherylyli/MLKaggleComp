{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import sklearn \n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "x_train = np.loadtxt(\"data/train_x.csv\", delimiter=\",\", dtype='uint8')\n",
    "y_train = np.loadtxt(\"data/train_y.csv\", delimiter=\",\", dtype='uint8')\n",
    "x_test = np.loadtxt(\"data/test_x.csv\", delimiter=\",\", dtype='uint8')\n",
    "\n",
    "# x_train = x.reshape(-1, 64, 64)\n",
    "# y_train = y.reshape(-1, 1) \n",
    "# x_test = x.reshape(-1, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 50000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), len(y_train))\n",
    "y_train_one_hot = [[0 for i in range(10)] for i in range(len(y_train))]\n",
    "for i in range(len(y_train)):\n",
    "    y_train_one_hot[i][int(y_train[i])] = 1\n",
    "   \n",
    "print(len(y_train_one_hot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 4096)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x181c9409e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.]\n"
     ]
    }
   ],
   "source": [
    "def show_img(img):\n",
    "    plt.close()\n",
    "    plt.imshow(np.uint8(img))\n",
    "    plt.show()\n",
    "\n",
    "x_train_reshaped = x_train.reshape(-1, 64, 64)\n",
    "y_train_reshaped = y_train.reshape(-1, 1) \n",
    "x_test_reshaped = x_test.reshape(-1, 64, 64)\n",
    "# scipy.misc.imshow(x_train_reshaped[0])\n",
    "show_img(x_train_reshaped[0])\n",
    "print(y_train_reshaped[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training data into train / valid sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train_s, x_valid_s, y_train_s, y_valid_s = train_test_split(x_train, y_train_one_hot, train_size=0.8, test_size=0.2)\n",
    "data = {\n",
    "    \"x_train\": x_train_s,\n",
    "    \"x_valid\": x_valid_s,\n",
    "    \"y_train\": y_train_s,\n",
    "    \"y_valid\": y_valid_s\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 4096)\n"
     ]
    }
   ],
   "source": [
    "# data[\"y_train\"] = data[\"y_train\"].astype(int)\n",
    "# data[\"y_valid\"] = data[\"y_valid\"].astype(int)\n",
    "# data[\"y_train\"] = data[\"y_train\"].T\n",
    "# data[\"y_valid\"] = data[\"y_valid\"].T\n",
    "print(data[\"x_train\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Linear Classifier: Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 4096)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0988\n"
     ]
    }
   ],
   "source": [
    "def baseline_linear_svm(data):\n",
    "    \"\"\"\n",
    "    Using out-of-the-box linear SVM to classify data\n",
    "    \"\"\"\n",
    "    clf = LinearSVC()\n",
    "    \n",
    "    y_pred = clf.fit(data[\"x_train\"], data[\"y_train\"]).predict(data[\"x_valid\"])\n",
    "    print(y_pred)\n",
    "    return metrics.accuracy_score(data[\"y_valid\"], y_pred, average=\"macro\"), y_pred\n",
    "    \n",
    "# score, y_pred = baseline_linear_svm(data)\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    sigmoid function\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    \"\"\"\n",
    "    derivative of sigmoid\n",
    "    \"\"\"\n",
    "    return x * (1. - x)\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"\n",
    "    tanh function\n",
    "    \"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def d_tanh(x):\n",
    "    \"\"\"\n",
    "    derivative of tanh\n",
    "    \"\"\"\n",
    "    return 1 - x*x\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    relu function\n",
    "    \"\"\"\n",
    "    return np.log(1 + np.exp(x))\n",
    "\n",
    "def d_relu(x):\n",
    "    \"\"\"\n",
    "    derivative of relu\n",
    "    \"\"\"\n",
    "    return np.exp(x) / 1. + np.exp(x)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    softmax classifier\n",
    "    40k x 10\n",
    "    \"\"\"\n",
    "    e = np.exp(x)\n",
    "    e_sum = np.sum(e, axis=1)\n",
    "    y = []\n",
    "    for i in range(len(e)):\n",
    "        y.append(e[i]/e_sum[i])\n",
    "    return np.array(y)\n",
    "\n",
    "def d_softmax(output, y):\n",
    "    \"\"\"\n",
    "    d loss / d output\n",
    "    \"\"\"\n",
    "    return output - y\n",
    "\n",
    "def log_loss(output):\n",
    "    loss = []\n",
    "    for i in range(len(output)):\n",
    "        log_loss_sum = []\n",
    "        for j in range(10):\n",
    "            log_loss_sum.append(np.log(output[i][j]))\n",
    "        loss.append(log_loss_sum)\n",
    "    return np.array(loss)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096, 200) (1, 200) (200, 10) (1, 10)\n"
     ]
    }
   ],
   "source": [
    "# # set number of hidden nodes\n",
    "# num_hidden_nodes = 200\n",
    "# num_input = 40000\n",
    "# input_size = 4096\n",
    "\n",
    "# # initialize random wh, bh, wo, and bo\n",
    "# wh = np.random.uniform(size=(input_size,num_hidden_nodes))\n",
    "# bh = np.random.uniform(size=(1, num_hidden_nodes))\n",
    "# wo = np.random.uniform(size=(num_hidden_nodes, 10))\n",
    "# bo = np.random.uniform(size=(1, 10))\n",
    "\n",
    "# print(wh.shape, bh.shape, wo.shape, bo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 200) (40000, 200) (40000, 10) (40000, 10) (40000, 10)\n",
      "[[ -0.00000000e+00  -4.87467870e-06  -0.00000000e+00 ...,  -0.00000000e+00\n",
      "   -0.00000000e+00  -0.00000000e+00]\n",
      " [ -0.00000000e+00  -0.00000000e+00  -0.00000000e+00 ...,  -0.00000000e+00\n",
      "   -0.00000000e+00  -0.00000000e+00]\n",
      " [ -0.00000000e+00  -0.00000000e+00  -0.00000000e+00 ...,  -2.62087981e+01\n",
      "   -0.00000000e+00  -0.00000000e+00]\n",
      " ..., \n",
      " [ -4.29758438e+02  -0.00000000e+00  -0.00000000e+00 ...,  -0.00000000e+00\n",
      "   -0.00000000e+00  -0.00000000e+00]\n",
      " [ -0.00000000e+00  -0.00000000e+00  -0.00000000e+00 ...,  -0.00000000e+00\n",
      "   -0.00000000e+00  -0.00000000e+00]\n",
      " [ -4.29758438e+02  -0.00000000e+00  -0.00000000e+00 ...,  -0.00000000e+00\n",
      "   -0.00000000e+00  -0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# hidden_layer_input = np.dot(data[\"x_train\"], wh) + bh\n",
    "# hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "# output_layer_input = np.dot(hidden_layer_output, wo) + bo\n",
    "# output_layer_output = softmax(output_layer_input)\n",
    "# error_output = log_loss(output_layer_output, data[\"y_train\"])\n",
    "\n",
    "# print(hidden_layer_input.shape, hidden_layer_output.shape, output_layer_input.shape, output_layer_output.shape, error_output.shape)\n",
    "# print(error_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[  7.22104758e-01   1.21599898e-02   3.64926004e-05   5.16692207e-07\n",
      "    2.34719011e-01   3.01892863e-02   6.58329379e-04   2.41881067e-11\n",
      "    3.21738840e-05   9.94423466e-05]\n",
      " [  7.22104758e-01   1.21599898e-02   3.64926004e-05   5.16692207e-07\n",
      "    2.34719011e-01   3.01892863e-02   6.58329379e-04   2.41881067e-11\n",
      "    3.21738840e-05   9.94423466e-05]\n",
      " [  7.22104758e-01   1.21599898e-02   3.64926004e-05   5.16692207e-07\n",
      "    2.34719011e-01   3.01892863e-02   6.58329379e-04   2.41881067e-11\n",
      "    3.21738840e-05   9.94423466e-05]\n",
      " [  7.22104758e-01   1.21599898e-02   3.64926004e-05   5.16692207e-07\n",
      "    2.34719011e-01   3.01892863e-02   6.58329379e-04   2.41881067e-11\n",
      "    3.21738840e-05   9.94423466e-05]\n",
      " [  7.22104758e-01   1.21599898e-02   3.64926004e-05   5.16692207e-07\n",
      "    2.34719011e-01   3.01892863e-02   6.58329379e-04   2.41881067e-11\n",
      "    3.21738840e-05   9.94423466e-05]\n",
      " [  7.22104758e-01   1.21599898e-02   3.64926004e-05   5.16692207e-07\n",
      "    2.34719011e-01   3.01892863e-02   6.58329379e-04   2.41881067e-11\n",
      "    3.21738840e-05   9.94423466e-05]\n",
      " [  7.22104758e-01   1.21599898e-02   3.64926004e-05   5.16692207e-07\n",
      "    2.34719011e-01   3.01892863e-02   6.58329379e-04   2.41881067e-11\n",
      "    3.21738840e-05   9.94423466e-05]\n",
      " [  7.22104758e-01   1.21599898e-02   3.64926004e-05   5.16692207e-07\n",
      "    2.34719011e-01   3.01892863e-02   6.58329379e-04   2.41881067e-11\n",
      "    3.21738840e-05   9.94423466e-05]\n",
      " [  7.22104758e-01   1.21599898e-02   3.64926004e-05   5.16692207e-07\n",
      "    2.34719011e-01   3.01892863e-02   6.58329379e-04   2.41881067e-11\n",
      "    3.21738840e-05   9.94423466e-05]\n",
      " [  7.22104758e-01   1.21599898e-02   3.64926004e-05   5.16692207e-07\n",
      "    2.34719011e-01   3.01892863e-02   6.58329379e-04   2.41881067e-11\n",
      "    3.21738840e-05   9.94423466e-05]]\n",
      "[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(output_layer_output[0]))\n",
    "print(output_layer_output[:10])\n",
    "print(data[\"y_train\"][:10])\n",
    "# print(data[\"y_train\"][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = -0.01\n",
    "\n",
    "# d_loss_d_output = d_softmax(output_layer_output, data[\"y_train\"])\n",
    "# d_loss_d_wo = np.dot(d_loss_d_output.T,  hidden_layer_output)\n",
    "\n",
    "# d_loss_d_hidden_output = np.dot(wo, d_loss_d_output.T)\n",
    "\n",
    "# print(d_loss_d_output.shape, d_loss_d_wo.shape, d_loss_d_hidden_output.shape)\n",
    "\n",
    "\n",
    "# slope_output_layer = d_softmax(output_layer_output, data[\"y_train\"])\n",
    "# slope_hidden_layer = d_sigmoid(hidden_layer_output)\n",
    "# d_output =  slope_output_layer * error_output * learning_rate\n",
    "\n",
    "# error_hidden = np.dot(d_output, wo.T)\n",
    "# d_hidden = error_hidden * slope_hidden_layer\n",
    "\n",
    "# wo = wo + np.dot(hidden_layer_output.T, d_output) * learning_rate\n",
    "# wh = wh + np.dot(data[\"x_train\"].T, d_hidden) * learning_rate\n",
    "# bo = bo + np.sum(d_output, axis=0) * learning_rate\n",
    "# bh = bh + np.sum(d_hidden, axis=0) * learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_rows, input_cols, learning_rate=0.01, num_nodes=200, activation_func=\"sigmoid\"):\n",
    "        self.input_rows = input_rows\n",
    "        self.input_cols = input_cols\n",
    "        self.num_nodes = num_nodes\n",
    "        self.next = None\n",
    "        self.prev = None\n",
    "        self.w = np.random.uniform(size=(self.input_cols,num_nodes))\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if activation_func == \"sigmoid\":\n",
    "            self.activation_func = self.sigmoid\n",
    "            self.d_activation_func = self.d_sigmoid\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    def feedforward(self, x):\n",
    "        \"\"\"\n",
    "        return the predictions (represented by a probability)\n",
    "        \"\"\"\n",
    "        # calculate stuff\n",
    "        before_activation = np.dot(x, w)\n",
    "        self.output = self.activation_func(before_activation) \n",
    "        self.derivative = self.d_activation_func(before_activation)\n",
    "        \n",
    "        \n",
    "        # if there's a next layer\n",
    "        if self.next:\n",
    "            # add bias to the end of each row of self.output\n",
    "            for i in range(self.input_rows):\n",
    "                self.output[i] = self.output[i].append(1)\n",
    "            \n",
    "            # call next layer's feedforward step\n",
    "            self.next.feedforward(self.output)\n",
    "\n",
    "        \n",
    "    def backprop(self, prev_deltas):\n",
    "        \"\"\"\n",
    "        compute derivatives and adjust w\n",
    "        \"\"\"\n",
    "        \n",
    "        deltas = np.dot(self.derivative, prev_deltas)\n",
    "        self.prev.backprop(np.dot(self.w, deltas))\n",
    "        self.w = self.w - self.learning_rate * np.dot(self.output, deltas)\n",
    "    \n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        sigmoid function\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def d_sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        derivative of sigmoid\n",
    "        \"\"\"\n",
    "        return x * (1. - x)\n",
    "    \n",
    "class OutputLayer(Layer):\n",
    "    def __init__(self, input_rows, input_cols, learning_rate=0.01, num_nodes=200, activation_func=\"softmax\", error_func=\"squared_error\"):\n",
    "        self.input_rows = input_rows\n",
    "        self.input_cols = input_cols\n",
    "        self.num_nodes = num_nodes\n",
    "        self.next = None\n",
    "        self.prev = None\n",
    "        self.w = np.random.uniform(size=(self.input_cols,num_nodes))\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if activation_func == \"softmax\":\n",
    "            self.activation_func = self.softmax\n",
    "            self.d_activation_func = self.d_softmax\n",
    "\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        if error_func == \"squared_error\":\n",
    "            self.error_func = (lambda x, target: (1/2) * np.squared(x - target))\n",
    "            self.d_error_func = lambda x, target: x - target\n",
    "        \n",
    "        \n",
    "    def backprop(self, targets):\n",
    "        deltas = self.derivative * self.d_error_func(self.output, targets)\n",
    "        self.prev.backprop(np.dot(self.w, deltas))\n",
    "        self.w = self.w - self.learning_rate * np.dot(self.output, deltas)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        e = np.exp(x)\n",
    "        e_sum = np.sum(e, axis=1)\n",
    "        y = []\n",
    "        for i in range(len(e)):\n",
    "            y.append(e[i]/e_sum[i])\n",
    "        return np.array(y)\n",
    "\n",
    "    def d_softmax(self, x):\n",
    "        \n",
    "    \n",
    "class NeuralNet:\n",
    "    def __init__(self, learning_rate, num_epochs):\n",
    "        self.first = None\n",
    "        self.last = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "    def add_layer(self, layer):\n",
    "        \"\"\"\n",
    "        Add layer to the end\n",
    "        \"\"\"\n",
    "        if not self.first:\n",
    "            self.first = layer\n",
    "            self.last = layer\n",
    "            \n",
    "        else:\n",
    "            temp = self.last\n",
    "            temp.next = layer\n",
    "            layer.prev = temp\n",
    "            self.last = layer\n",
    "            \n",
    "    def fit(self, x_train, y_train):\n",
    "        x_input = []\n",
    "        # add bias to x\n",
    "        for i in range(len(x_train)):\n",
    "            x_input.append(x[i].append(1))\n",
    "            \n",
    "        for i in range(self.num_epochs):\n",
    "            self.first.feedforward(x_input)\n",
    "            self.last.backprop(y_train)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error at i = 0 -5440394.06185\n",
      "[[  94.40669398  104.89006279   90.80468338  106.35614204   95.30846331\n",
      "   113.42472528   99.04346378   98.6508905    96.01620947   99.3465586 ]\n",
      " [  94.40669398  104.89006279   90.80468338  106.35614204   95.30846331\n",
      "   113.42472528   99.04346378   98.6508905    96.01620947   99.3465586 ]\n",
      " [  94.40669398  104.89006279   90.80468338  106.35614204   95.30846331\n",
      "   113.42472528   99.04346378   98.6508905    96.01620947   99.3465586 ]]\n",
      "[[  5.49690578e-09   1.96330386e-04   1.49894247e-10   8.50545684e-04\n",
      "    1.35441495e-08   9.98951359e-01   5.67337947e-07   3.83132879e-07\n",
      "    2.74866612e-08   7.68199888e-07]\n",
      " [  5.49690578e-09   1.96330386e-04   1.49894247e-10   8.50545684e-04\n",
      "    1.35441495e-08   9.98951359e-01   5.67337947e-07   3.83132879e-07\n",
      "    2.74866612e-08   7.68199888e-07]\n",
      " [  5.49690578e-09   1.96330386e-04   1.49894247e-10   8.50545684e-04\n",
      "    1.35441495e-08   9.98951359e-01   5.67337947e-07   3.83132879e-07\n",
      "    2.74866612e-08   7.68199888e-07]]\n",
      "error at i = 10 -924870.91148\n",
      "[[ 113.12732948  112.69396486  113.10811974  112.89870506  113.08751214\n",
      "   112.88006273  113.10175337  113.12530813  113.15966164  113.06465864]\n",
      " [ 113.12732948  112.69396486  113.10811974  112.89870506  113.08751214\n",
      "   112.88006273  113.10175337  113.12530813  113.15966164  113.06465864]\n",
      " [ 113.12732948  112.69396486  113.10811974  112.89870506  113.08751214\n",
      "   112.88006273  113.10175337  113.12530813  113.15966164  113.06465864]]\n",
      "[[ 0.10974943  0.0711532   0.1076613   0.08731965  0.10546536  0.08570688\n",
      "   0.10697806  0.10952782  0.11335586  0.10308244]\n",
      " [ 0.10974943  0.0711532   0.1076613   0.08731965  0.10546536  0.08570688\n",
      "   0.10697806  0.10952782  0.11335586  0.10308244]\n",
      " [ 0.10974943  0.0711532   0.1076613   0.08731965  0.10546536  0.08570688\n",
      "   0.10697806  0.10952782  0.11335586  0.10308244]]\n",
      "error at i = 20 -921680.415031\n",
      "[[ 113.18042494  113.02867115  113.16807114  113.1536146   113.15511921\n",
      "   113.08456835  113.18892818  113.23748586  113.21154818  113.15369847]\n",
      " [ 113.18042494  113.02867115  113.16807114  113.1536146   113.15511921\n",
      "   113.08456835  113.18892818  113.23748586  113.21154818  113.15369847]\n",
      " [ 113.18042494  113.02867115  113.16807114  113.1536146   113.15511921\n",
      "   113.08456835  113.18892818  113.23748586  113.21154818  113.15369847]]\n",
      "[[ 0.10228532  0.08788353  0.10102948  0.09957945  0.09972939  0.09293586\n",
      "   0.10315879  0.10829155  0.10551883  0.0995878 ]\n",
      " [ 0.10228532  0.08788353  0.10102948  0.09957945  0.09972939  0.09293586\n",
      "   0.10315879  0.10829155  0.10551883  0.0995878 ]\n",
      " [ 0.10228532  0.08788353  0.10102948  0.09957945  0.09972939  0.09293586\n",
      "   0.10315879  0.10829155  0.10551883  0.0995878 ]]\n",
      "error at i = 30 -921284.647466\n",
      "[[ 113.19933606  113.16287128  113.18951215  113.20990457  113.17900025\n",
      "   113.12334824  113.21194117  113.26143875  113.22568633  113.18028587]\n",
      " [ 113.19933606  113.16287128  113.18951215  113.20990457  113.17900025\n",
      "   113.12334824  113.21194117  113.26143875  113.22568633  113.18028587]\n",
      " [ 113.19933606  113.16287128  113.18951215  113.20990457  113.17900025\n",
      "   113.12334824  113.21194117  113.26143875  113.22568633  113.18028587]]\n",
      "[[ 0.10043867  0.09684216  0.0994568   0.10150578  0.09841679  0.09308932\n",
      "   0.10171272  0.10687393  0.10312043  0.0985434 ]\n",
      " [ 0.10043867  0.09684216  0.0994568   0.10150578  0.09841679  0.09308932\n",
      "   0.10171272  0.10687393  0.10312043  0.0985434 ]\n",
      " [ 0.10043867  0.09684216  0.0994568   0.10150578  0.09841679  0.09308932\n",
      "   0.10171272  0.10687393  0.10312043  0.0985434 ]]\n",
      "error at i = 40 -921252.775686\n",
      "[[ 113.20598236  113.23148344  113.19703192  113.22320619  113.18730637\n",
      "   113.13448792  113.21862741  113.26554165  113.22983774  113.18890017]\n",
      " [ 113.20598236  113.23148344  113.19703192  113.22320619  113.18730637\n",
      "   113.13448792  113.21862741  113.26554165  113.22983774  113.18890017]\n",
      " [ 113.20598236  113.23148344  113.19703192  113.22320619  113.18730637\n",
      "   113.13448792  113.21862741  113.26554165  113.22983774  113.18890017]]\n",
      "[[ 0.09971989  0.10229556  0.09883134  0.10145233  0.09787481  0.09283936\n",
      "   0.10098886  0.10583957  0.10212735  0.09803092]\n",
      " [ 0.09971989  0.10229556  0.09883134  0.10145233  0.09787481  0.09283936\n",
      "   0.10098886  0.10583957  0.10212735  0.09803092]\n",
      " [ 0.09971989  0.10229556  0.09883134  0.10145233  0.09787481  0.09283936\n",
      "   0.10098886  0.10583957  0.10212735  0.09803092]]\n",
      "error at i = 50 -921298.827528\n",
      "[[ 113.20761607  113.26996708  113.19902405  113.22570528  113.18962088\n",
      "   113.13788343  113.21991984  113.26448688  113.23025826  113.19122421]\n",
      " [ 113.20761607  113.26996708  113.19902405  113.22570528  113.18962088\n",
      "   113.13788343  113.21991984  113.26448688  113.23025826  113.19122421]\n",
      " [ 113.20761607  113.26996708  113.19902405  113.22570528  113.18962088\n",
      "   113.13788343  113.21991984  113.26448688  113.23025826  113.19122421]]\n",
      "[[ 0.09934053  0.10573169  0.09849065  0.10115388  0.09756887  0.09264927\n",
      "   0.10057035  0.10515385  0.10161548  0.09772543]\n",
      " [ 0.09934053  0.10573169  0.09849065  0.10115388  0.09756887  0.09264927\n",
      "   0.10057035  0.10515385  0.10161548  0.09772543]\n",
      " [ 0.09934053  0.10573169  0.09849065  0.10115388  0.09756887  0.09264927\n",
      "   0.10057035  0.10515385  0.10161548  0.09772543]]\n"
     ]
    }
   ],
   "source": [
    "def train_mlp(data):\n",
    "    # set number of hidden nodes\n",
    "    num_hidden_nodes = 200\n",
    "    num_input = 40000\n",
    "    input_size = 4096\n",
    "    learning_rate = 0.001\n",
    "    num_epoch = 51\n",
    "\n",
    "    # initialize random wh, bh, wo, and bo\n",
    "    wh = np.random.uniform(size=(input_size,num_hidden_nodes))\n",
    "    bh = np.random.uniform(size=(1, num_hidden_nodes))\n",
    "    wo = np.random.uniform(size=(num_hidden_nodes, 10))\n",
    "    bo = np.random.uniform(size=(1, 10))\n",
    "\n",
    "    \n",
    "    for i in range(num_epoch):\n",
    "        hidden_layer_input = np.dot(data[\"x_train\"], wh) + bh\n",
    "        hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "        output_layer_input = np.dot(hidden_layer_output, wo) + bo\n",
    "        output_layer_output = softmax(output_layer_input)\n",
    "        error_output = log_loss(output_layer_output)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(\"error at i =\", i,  np.sum(error_output))\n",
    "            print(output_layer_input[:3])\n",
    "            print(output_layer_output[:3])\n",
    "#             print(wo[0], wh[0], bo[0], bh[0])\n",
    "\n",
    "\n",
    "        slope_output_layer = d_softmax(output_layer_output, data[\"y_train\"])\n",
    "        slope_hidden_layer = d_sigmoid(hidden_layer_output)\n",
    "        d_output = error_output * slope_output_layer * learning_rate\n",
    "\n",
    "        error_hidden = np.dot(d_output, wo.T)\n",
    "        d_hidden = error_hidden * slope_hidden_layer\n",
    "\n",
    "        wo = wo + np.dot(hidden_layer_output.T, d_output) * learning_rate\n",
    "        wh = wh + np.dot(data[\"x_train\"].T, d_hidden) * learning_rate\n",
    "        bo = bo + np.sum(d_output, axis=0) * learning_rate\n",
    "        bh = bh + np.sum(d_hidden, axis=0) * learning_rate\n",
    "        \n",
    "    model = {\"wh\": wh, \"bh\": bh, \"wo\": wo, \"bo\": bo}\n",
    "        \n",
    "    return model\n",
    "\n",
    "model = train_mlp(data)\n",
    "\n",
    "# def predict(x, model, y):\n",
    "#     hidden_input = np.dot(x, model[\"wh\"]) + model[\"bh\"]\n",
    "#     hidden_output = sigmoid(hidden_input)\n",
    "#     outer_input = np.dot(hidden_output, model['wo']) + model['bo']\n",
    "#     outer_ouput = sigmoid(outer_input)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
