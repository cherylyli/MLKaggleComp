{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import sklearn \n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "x_train = np.loadtxt(\"data/train_x.csv\", delimiter=\",\")\n",
    "y_train = np.loadtxt(\"data/train_y.csv\", delimiter=\",\")\n",
    "x_test = np.loadtxt(\"data/test_x.csv\", delimiter=\",\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode y_train data in one-hot encoding\n",
    "y_train_one_hot = [[0 for i in range(10)] for i in range(len(y_train))]\n",
    "for i in range(len(y_train)):\n",
    "    y_train_one_hot[i][int(y_train[i])] = 1\n",
    "\n",
    "y_train_one_hot = np.array(y_train_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg4SAz8IOkgM/CDoIP8fMscSGhaE3RgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x193225ef28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.]\n"
     ]
    }
   ],
   "source": [
    "def show_img(img):\n",
    "    plt.close()\n",
    "    plt.imshow(img, cmap='gray_r')\n",
    "    plt.show()\n",
    "\n",
    "x_train_reshaped = x_train.reshape(-1, 64, 64)\n",
    "y_train_reshaped = y_train.reshape(-1, 1) \n",
    "x_test_reshaped = x_test.reshape(-1, 64, 64)\n",
    "\n",
    "show_img(x_train_reshaped[0])\n",
    "print(y_train_reshaped[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training data into train / valid sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train_s, x_valid_s, y_train_s, y_valid_s = train_test_split(x_train, y_train_one_hot, train_size=0.8, test_size=0.2)\n",
    "data = {\n",
    "    \"x_train\": x_train_s,\n",
    "    \"x_valid\": x_valid_s,\n",
    "    \"y_train\": y_train_s,\n",
    "    \"y_valid\": y_valid_s\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"y_train\"] = data[\"y_train\"].astype(int)\n",
    "# data[\"y_valid\"] = data[\"y_valid\"].astype(int)\n",
    "# data[\"y_train\"] = data[\"y_train\"].T\n",
    "# data[\"y_valid\"] = data[\"y_valid\"].T\n",
    "# print(data[\"x_train\"].shape)\n",
    "# print(data[\"y_train\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Linear Classifier: Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 4096)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0988\n"
     ]
    }
   ],
   "source": [
    "def baseline_linear_svm(data):\n",
    "    \"\"\"\n",
    "    Using out-of-the-box linear SVM to classify data\n",
    "    \"\"\"\n",
    "    clf = LinearSVC()\n",
    "    \n",
    "    y_pred = clf.fit(data[\"x_train\"], data[\"y_train\"]).predict(data[\"x_valid\"])\n",
    "    print(y_pred)\n",
    "    return metrics.accuracy_score(data[\"y_valid\"], y_pred, average=\"macro\"), y_pred\n",
    "    \n",
    "# score, y_pred = baseline_linear_svm(data)\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    sigmoid function\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    \"\"\"\n",
    "    derivative of sigmoid\n",
    "    \"\"\"\n",
    "    return x * (1. - x)\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"\n",
    "    tanh function\n",
    "    \"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def d_tanh(x):\n",
    "    \"\"\"\n",
    "    derivative of tanh\n",
    "    \"\"\"\n",
    "    return 1 - x*x\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    relu function\n",
    "    \"\"\"\n",
    "    return np.log(1 + np.exp(x))\n",
    "\n",
    "def d_relu(x):\n",
    "    \"\"\"\n",
    "    derivative of relu\n",
    "    \"\"\"\n",
    "    return np.exp(x) / 1. + np.exp(x)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    softmax classifier\n",
    "    40k x 10\n",
    "    \"\"\"\n",
    "    e = np.exp(x)\n",
    "    e_sum = np.sum(e, axis=1)\n",
    "    y = []\n",
    "    for i in range(len(e)):\n",
    "        y.append(e[i]/e_sum[i])\n",
    "    return np.array(y)\n",
    "\n",
    "def d_softmax(output, y):\n",
    "    \"\"\"\n",
    "    d loss / d output\n",
    "    \"\"\"\n",
    "    return output - y\n",
    "\n",
    "def log_loss(output):\n",
    "    loss = []\n",
    "    for i in range(len(output)):\n",
    "        log_loss_sum = []\n",
    "        for j in range(10):\n",
    "            log_loss_sum.append(np.log(output[i][j]))\n",
    "        loss.append(log_loss_sum)\n",
    "    return np.array(loss)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cheryl/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:54: RuntimeWarning: overflow encountered in exp\n",
      "/Users/cheryl/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:47: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'first' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5cff2eaba9f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0mneural_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m \u001b[0mpredict_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneural_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"x_valid\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-5cff2eaba9f6>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mx_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mfirst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'first' is not defined"
     ]
    }
   ],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_rows, input_cols, learning_rate=0.01, num_nodes=200, activation_func=\"sigmoid\"):\n",
    "        self.input_rows = input_rows\n",
    "        self.input_cols = input_cols\n",
    "        self.num_nodes = num_nodes\n",
    "        self.next = None\n",
    "        self.prev = None\n",
    "        self.w = np.random.uniform(size=(self.input_cols,num_nodes))\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if activation_func == \"sigmoid\":\n",
    "            self.activation_func = self.sigmoid\n",
    "            self.d_activation_func = self.d_sigmoid\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    def feedforward(self, x):\n",
    "        \"\"\"\n",
    "        return the predictions (represented by a probability)\n",
    "        \"\"\"\n",
    "        # calculate stuff\n",
    "        before_activation = np.dot(x, self.w)\n",
    "        self.output = self.activation_func(before_activation) \n",
    "        self.derivative = self.d_activation_func(before_activation)\n",
    "        \n",
    "        \n",
    "        # if there's a next layer\n",
    "        if self.next:\n",
    "            # add bias to the end of each row of self.output\n",
    "            try:\n",
    "                self.output = np.append(self.output, np.ones((self.output.shape[0], 1)), axis=-1)\n",
    "            except ValueError:\n",
    "                self.output = np.append(self.output, 1) \n",
    "            \n",
    "            # call next layer's feedforward step\n",
    "            self.next.feedforward(self.output)\n",
    "\n",
    "        \n",
    "    def backprop(self, prev_deltas):\n",
    "        \"\"\"\n",
    "        compute derivatives and adjust w\n",
    "        \"\"\" \n",
    "        deltas = self.derivative * prev_deltas \n",
    "        if self.prev:\n",
    "            self.prev.backprop(np.dot(self.w[:-1], deltas))\n",
    "        self.w = self.w - (self.learning_rate * self.output[:-1] * deltas)\n",
    "    \n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        sigmoid function\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def d_sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        derivative of sigmoid\n",
    "        \"\"\"\n",
    "        return x * (1. - x)\n",
    "   \n",
    "    \n",
    "class OutputLayer(Layer):\n",
    "    def __init__(self, input_rows, input_cols, learning_rate=0.01, num_nodes=200, activation_func=\"softmax\"):\n",
    "        self.input_rows = input_rows\n",
    "        self.input_cols = input_cols\n",
    "        self.num_nodes = num_nodes\n",
    "        self.next = None\n",
    "        self.prev = None\n",
    "        self.w = np.random.uniform(size=(self.input_cols,num_nodes))\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if activation_func == \"softmax\":\n",
    "            self.activation_func = self.softmax\n",
    "            self.backprop_func = lambda x, target: x - target\n",
    "            \n",
    "        elif activation_func == \"sigmoid\":\n",
    "            self.activation_func = self.sigmoid\n",
    "            self.backprop_func = lambda x, target: self.d_sigmoid(x) * (x - target)\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # Value is unused.\n",
    "        self.d_activation_func = lambda x: None\n",
    "     \n",
    "    def backprop(self, targets):\n",
    "        deltas = self.backprop_func(self.output, targets)\n",
    "        self.prev.backprop(np.dot(self.w[:-1], deltas))\n",
    "        self.w = self.w - self.learning_rate * np.dot(self.output, deltas)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        e = np.exp(x - np.amax(x))\n",
    "        dist = e / np.sum(e)\n",
    "        return dist\n",
    "        # e = np.exp(x)\n",
    "        # e_sum = np.sum(e, axis=1)\n",
    "        # y = []\n",
    "        # for i in range(len(e)):\n",
    "        #     y.append(e[i]/e_sum[i])\n",
    "        # return np.array(y)\n",
    "        \n",
    "    \n",
    "class NeuralNet:\n",
    "    def __init__(self, learning_rate, num_epochs):\n",
    "        self.first = None\n",
    "        self.last = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "    def add_layer(self, layer):\n",
    "        \"\"\"\n",
    "        Add layer to the end\n",
    "        \"\"\"\n",
    "        if not self.first:\n",
    "            self.first = layer\n",
    "            self.last = layer\n",
    "            \n",
    "        else:\n",
    "            temp = self.last\n",
    "            temp.next = layer\n",
    "            layer.prev = temp\n",
    "            self.last = layer\n",
    "            \n",
    "    def fit(self, x_train, y_train):\n",
    "        x_input = np.append(x_train, np.ones((x_train.shape[0], 1)), axis=-1)\n",
    "        \n",
    "        for i in range(self.num_epochs): \n",
    "            for j in range(len(x_input)):\n",
    "                self.first.feedforward(x_input[j])\n",
    "                self.last.backprop(y_train[j])\n",
    "                \n",
    "    def predict(self, x):\n",
    "        x_input = np.append(x, np.ones((x.shape[0], 1)), axis=-1)\n",
    "        self.first.feedforward(x_input)\n",
    "        return self.last.output\n",
    "\n",
    "\n",
    "x_tr = data[\"x_train\"]\n",
    "y_tr = data[\"y_train\"]\n",
    "\n",
    "neural_net = NeuralNet(0.1, 10)\n",
    "\n",
    "neural_net.add_layer(Layer(x_tr.shape[0], x_tr.shape[1] + 1, 0.1, 300))\n",
    "neural_net.add_layer(Layer(x_tr.shape[0], 300 + 1, 0.1, 200))\n",
    "neural_net.add_layer(OutputLayer(x_tr.shape[0], 200 + 1, 0.1, 10))\n",
    "\n",
    "neural_net.fit(x_tr, y_tr)\n",
    "predict_y = neural_net.predict(data[\"x_valid\"])\n",
    "print(\"Done\")\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error at i = 0 -5440394.06185\n",
      "[[  94.40669398  104.89006279   90.80468338  106.35614204   95.30846331\n",
      "   113.42472528   99.04346378   98.6508905    96.01620947   99.3465586 ]\n",
      " [  94.40669398  104.89006279   90.80468338  106.35614204   95.30846331\n",
      "   113.42472528   99.04346378   98.6508905    96.01620947   99.3465586 ]\n",
      " [  94.40669398  104.89006279   90.80468338  106.35614204   95.30846331\n",
      "   113.42472528   99.04346378   98.6508905    96.01620947   99.3465586 ]]\n",
      "[[  5.49690578e-09   1.96330386e-04   1.49894247e-10   8.50545684e-04\n",
      "    1.35441495e-08   9.98951359e-01   5.67337947e-07   3.83132879e-07\n",
      "    2.74866612e-08   7.68199888e-07]\n",
      " [  5.49690578e-09   1.96330386e-04   1.49894247e-10   8.50545684e-04\n",
      "    1.35441495e-08   9.98951359e-01   5.67337947e-07   3.83132879e-07\n",
      "    2.74866612e-08   7.68199888e-07]\n",
      " [  5.49690578e-09   1.96330386e-04   1.49894247e-10   8.50545684e-04\n",
      "    1.35441495e-08   9.98951359e-01   5.67337947e-07   3.83132879e-07\n",
      "    2.74866612e-08   7.68199888e-07]]\n",
      "error at i = 10 -924870.91148\n",
      "[[ 113.12732948  112.69396486  113.10811974  112.89870506  113.08751214\n",
      "   112.88006273  113.10175337  113.12530813  113.15966164  113.06465864]\n",
      " [ 113.12732948  112.69396486  113.10811974  112.89870506  113.08751214\n",
      "   112.88006273  113.10175337  113.12530813  113.15966164  113.06465864]\n",
      " [ 113.12732948  112.69396486  113.10811974  112.89870506  113.08751214\n",
      "   112.88006273  113.10175337  113.12530813  113.15966164  113.06465864]]\n",
      "[[ 0.10974943  0.0711532   0.1076613   0.08731965  0.10546536  0.08570688\n",
      "   0.10697806  0.10952782  0.11335586  0.10308244]\n",
      " [ 0.10974943  0.0711532   0.1076613   0.08731965  0.10546536  0.08570688\n",
      "   0.10697806  0.10952782  0.11335586  0.10308244]\n",
      " [ 0.10974943  0.0711532   0.1076613   0.08731965  0.10546536  0.08570688\n",
      "   0.10697806  0.10952782  0.11335586  0.10308244]]\n",
      "error at i = 20 -921680.415031\n",
      "[[ 113.18042494  113.02867115  113.16807114  113.1536146   113.15511921\n",
      "   113.08456835  113.18892818  113.23748586  113.21154818  113.15369847]\n",
      " [ 113.18042494  113.02867115  113.16807114  113.1536146   113.15511921\n",
      "   113.08456835  113.18892818  113.23748586  113.21154818  113.15369847]\n",
      " [ 113.18042494  113.02867115  113.16807114  113.1536146   113.15511921\n",
      "   113.08456835  113.18892818  113.23748586  113.21154818  113.15369847]]\n",
      "[[ 0.10228532  0.08788353  0.10102948  0.09957945  0.09972939  0.09293586\n",
      "   0.10315879  0.10829155  0.10551883  0.0995878 ]\n",
      " [ 0.10228532  0.08788353  0.10102948  0.09957945  0.09972939  0.09293586\n",
      "   0.10315879  0.10829155  0.10551883  0.0995878 ]\n",
      " [ 0.10228532  0.08788353  0.10102948  0.09957945  0.09972939  0.09293586\n",
      "   0.10315879  0.10829155  0.10551883  0.0995878 ]]\n",
      "error at i = 30 -921284.647466\n",
      "[[ 113.19933606  113.16287128  113.18951215  113.20990457  113.17900025\n",
      "   113.12334824  113.21194117  113.26143875  113.22568633  113.18028587]\n",
      " [ 113.19933606  113.16287128  113.18951215  113.20990457  113.17900025\n",
      "   113.12334824  113.21194117  113.26143875  113.22568633  113.18028587]\n",
      " [ 113.19933606  113.16287128  113.18951215  113.20990457  113.17900025\n",
      "   113.12334824  113.21194117  113.26143875  113.22568633  113.18028587]]\n",
      "[[ 0.10043867  0.09684216  0.0994568   0.10150578  0.09841679  0.09308932\n",
      "   0.10171272  0.10687393  0.10312043  0.0985434 ]\n",
      " [ 0.10043867  0.09684216  0.0994568   0.10150578  0.09841679  0.09308932\n",
      "   0.10171272  0.10687393  0.10312043  0.0985434 ]\n",
      " [ 0.10043867  0.09684216  0.0994568   0.10150578  0.09841679  0.09308932\n",
      "   0.10171272  0.10687393  0.10312043  0.0985434 ]]\n",
      "error at i = 40 -921252.775686\n",
      "[[ 113.20598236  113.23148344  113.19703192  113.22320619  113.18730637\n",
      "   113.13448792  113.21862741  113.26554165  113.22983774  113.18890017]\n",
      " [ 113.20598236  113.23148344  113.19703192  113.22320619  113.18730637\n",
      "   113.13448792  113.21862741  113.26554165  113.22983774  113.18890017]\n",
      " [ 113.20598236  113.23148344  113.19703192  113.22320619  113.18730637\n",
      "   113.13448792  113.21862741  113.26554165  113.22983774  113.18890017]]\n",
      "[[ 0.09971989  0.10229556  0.09883134  0.10145233  0.09787481  0.09283936\n",
      "   0.10098886  0.10583957  0.10212735  0.09803092]\n",
      " [ 0.09971989  0.10229556  0.09883134  0.10145233  0.09787481  0.09283936\n",
      "   0.10098886  0.10583957  0.10212735  0.09803092]\n",
      " [ 0.09971989  0.10229556  0.09883134  0.10145233  0.09787481  0.09283936\n",
      "   0.10098886  0.10583957  0.10212735  0.09803092]]\n",
      "error at i = 50 -921298.827528\n",
      "[[ 113.20761607  113.26996708  113.19902405  113.22570528  113.18962088\n",
      "   113.13788343  113.21991984  113.26448688  113.23025826  113.19122421]\n",
      " [ 113.20761607  113.26996708  113.19902405  113.22570528  113.18962088\n",
      "   113.13788343  113.21991984  113.26448688  113.23025826  113.19122421]\n",
      " [ 113.20761607  113.26996708  113.19902405  113.22570528  113.18962088\n",
      "   113.13788343  113.21991984  113.26448688  113.23025826  113.19122421]]\n",
      "[[ 0.09934053  0.10573169  0.09849065  0.10115388  0.09756887  0.09264927\n",
      "   0.10057035  0.10515385  0.10161548  0.09772543]\n",
      " [ 0.09934053  0.10573169  0.09849065  0.10115388  0.09756887  0.09264927\n",
      "   0.10057035  0.10515385  0.10161548  0.09772543]\n",
      " [ 0.09934053  0.10573169  0.09849065  0.10115388  0.09756887  0.09264927\n",
      "   0.10057035  0.10515385  0.10161548  0.09772543]]\n"
     ]
    }
   ],
   "source": [
    "def train_mlp(data):\n",
    "    # set number of hidden nodes\n",
    "    num_hidden_nodes = 200\n",
    "    num_input = 40000\n",
    "    input_size = 4096\n",
    "    learning_rate = 0.001\n",
    "    num_epoch = 51\n",
    "\n",
    "    # initialize random wh, bh, wo, and bo\n",
    "    wh = np.random.uniform(size=(input_size,num_hidden_nodes))\n",
    "    bh = np.random.uniform(size=(1, num_hidden_nodes))\n",
    "    wo = np.random.uniform(size=(num_hidden_nodes, 10))\n",
    "    bo = np.random.uniform(size=(1, 10))\n",
    "\n",
    "    \n",
    "    for i in range(num_epoch):\n",
    "        hidden_layer_input = np.dot(data[\"x_train\"], wh) + bh\n",
    "        hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "        output_layer_input = np.dot(hidden_layer_output, wo) + bo\n",
    "        output_layer_output = softmax(output_layer_input)\n",
    "        error_output = log_loss(output_layer_output)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(\"error at i =\", i,  np.sum(error_output))\n",
    "            print(output_layer_input[:3])\n",
    "            print(output_layer_output[:3])\n",
    "#             print(wo[0], wh[0], bo[0], bh[0])\n",
    "\n",
    "\n",
    "        slope_output_layer = d_softmax(output_layer_output, data[\"y_train\"])\n",
    "        slope_hidden_layer = d_sigmoid(hidden_layer_output)\n",
    "        d_output = error_output * slope_output_layer * learning_rate\n",
    "\n",
    "        error_hidden = np.dot(d_output, wo.T)\n",
    "        d_hidden = error_hidden * slope_hidden_layer\n",
    "\n",
    "        wo = wo + np.dot(hidden_layer_output.T, d_output) * learning_rate\n",
    "        wh = wh + np.dot(data[\"x_train\"].T, d_hidden) * learning_rate\n",
    "        bo = bo + np.sum(d_output, axis=0) * learning_rate\n",
    "        bh = bh + np.sum(d_hidden, axis=0) * learning_rate\n",
    "        \n",
    "    model = {\"wh\": wh, \"bh\": bh, \"wo\": wo, \"bo\": bo}\n",
    "        \n",
    "    return model\n",
    "\n",
    "model = train_mlp(data)\n",
    "\n",
    "# def predict(x, model, y):\n",
    "#     hidden_input = np.dot(x, model[\"wh\"]) + model[\"bh\"]\n",
    "#     hidden_output = sigmoid(hidden_input)\n",
    "#     outer_input = np.dot(hidden_output, model['wo']) + model['bo']\n",
    "#     outer_ouput = sigmoid(outer_input)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
