{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import sklearn \n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "x_train = np.loadtxt(\"data/train_x.csv\", delimiter=\",\")\n",
    "y_train = np.loadtxt(\"data/train_y.csv\", delimiter=\",\")\n",
    "x_test = np.loadtxt(\"data/test_x.csv\", delimiter=\",\")\n",
    "print(\"Done\")\n",
    "# x_train = x.reshape(-1, 64, 64)\n",
    "# y_train = y.reshape(-1, 1) \n",
    "# x_test = x.reshape(-1, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 50000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), len(y_train))\n",
    "y_train_one_hot = [[0 for i in range(10)] for i in range(len(y_train))]\n",
    "for i in range(len(y_train)):\n",
    "    y_train_one_hot[i][int(y_train[i])] = 1\n",
    "   \n",
    "print(len(y_train_one_hot))\n",
    "y_train_one_hot = np.array(y_train_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 4096)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.]\n"
     ]
    }
   ],
   "source": [
    "def show_img(img):\n",
    "    plt.close()\n",
    "    plt.imshow(img, cmap='gray_r')\n",
    "    plt.show()\n",
    "\n",
    "x_train_reshaped = x_train.reshape(-1, 64, 64)\n",
    "y_train_reshaped = y_train.reshape(-1, 1) \n",
    "x_test_reshaped = x_test.reshape(-1, 64, 64)\n",
    "# scipy.misc.imshow(x_train_reshaped[0])\n",
    "show_img(x_train_reshaped[0])\n",
    "print(y_train_reshaped[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training data into train / valid sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train_s, x_valid_s, y_train_s, y_valid_s = train_test_split(x_train, y_train_one_hot, train_size=0.8, test_size=0.2)\n",
    "data = {\n",
    "    \"x_train\": x_train_s,\n",
    "    \"x_valid\": x_valid_s,\n",
    "    \"y_train\": y_train_s,\n",
    "    \"y_valid\": y_valid_s\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 4096)\n(40000, 10)\n"
     ]
    }
   ],
   "source": [
    "# data[\"y_train\"] = data[\"y_train\"].astype(int)\n",
    "# data[\"y_valid\"] = data[\"y_valid\"].astype(int)\n",
    "# data[\"y_train\"] = data[\"y_train\"].T\n",
    "# data[\"y_valid\"] = data[\"y_valid\"].T\n",
    "print(data[\"x_train\"].shape)\n",
    "print(data[\"y_train\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Linear Classifier: Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 4096)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0988\n"
     ]
    }
   ],
   "source": [
    "def baseline_linear_svm(data):\n",
    "    \"\"\"\n",
    "    Using out-of-the-box linear SVM to classify data\n",
    "    \"\"\"\n",
    "    clf = LinearSVC()\n",
    "    \n",
    "    y_pred = clf.fit(data[\"x_train\"], data[\"y_train\"]).predict(data[\"x_valid\"])\n",
    "    print(y_pred)\n",
    "    return metrics.accuracy_score(data[\"y_valid\"], y_pred, average=\"macro\"), y_pred\n",
    "    \n",
    "# score, y_pred = baseline_linear_svm(data)\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    sigmoid function\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    \"\"\"\n",
    "    derivative of sigmoid\n",
    "    \"\"\"\n",
    "    return x * (1. - x)\n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"\n",
    "    tanh function\n",
    "    \"\"\"\n",
    "    return np.tanh(x)\n",
    "\n",
    "def d_tanh(x):\n",
    "    \"\"\"\n",
    "    derivative of tanh\n",
    "    \"\"\"\n",
    "    return 1 - x*x\n",
    "\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    relu function\n",
    "    \"\"\"\n",
    "    return np.log(1 + np.exp(x))\n",
    "\n",
    "def d_relu(x):\n",
    "    \"\"\"\n",
    "    derivative of relu\n",
    "    \"\"\"\n",
    "    return np.exp(x) / 1. + np.exp(x)\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    softmax classifier\n",
    "    40k x 10\n",
    "    \"\"\"\n",
    "    e = np.exp(x)\n",
    "    e_sum = np.sum(e, axis=1)\n",
    "    y = []\n",
    "    for i in range(len(e)):\n",
    "        y.append(e[i]/e_sum[i])\n",
    "    return np.array(y)\n",
    "\n",
    "def d_softmax(output, y):\n",
    "    \"\"\"\n",
    "    d loss / d output\n",
    "    \"\"\"\n",
    "    return output - y\n",
    "\n",
    "def log_loss(output):\n",
    "    loss = []\n",
    "    for i in range(len(output)):\n",
    "        log_loss_sum = []\n",
    "        for j in range(10):\n",
    "            log_loss_sum.append(np.log(output[i][j]))\n",
    "        loss.append(log_loss_sum)\n",
    "    return np.array(loss)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096, 200) (1, 200) (200, 10) (1, 10)\n"
     ]
    }
   ],
   "source": [
    "# # set number of hidden nodes\n",
    "# num_hidden_nodes = 200\n",
    "# num_input = 40000\n",
    "# input_size = 4096\n",
    "\n",
    "# # initialize random wh, bh, wo, and bo\n",
    "# wh = np.random.uniform(size=(input_size,num_hidden_nodes))\n",
    "# bh = np.random.uniform(size=(1, num_hidden_nodes))\n",
    "# wo = np.random.uniform(size=(num_hidden_nodes, 10))\n",
    "# bo = np.random.uniform(size=(1, 10))\n",
    "\n",
    "# print(wh.shape, bh.shape, wo.shape, bo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 200) (40000, 200) (40000, 10) (40000, 10) (40000, 10)\n",
      "[[ -0.00000000e+00  -4.87467870e-06  -0.00000000e+00 ...,  -0.00000000e+00\n",
      "   -0.00000000e+00  -0.00000000e+00]\n",
      " [ -0.00000000e+00  -0.00000000e+00  -0.00000000e+00 ...,  -0.00000000e+00\n",
      "   -0.00000000e+00  -0.00000000e+00]\n",
      " [ -0.00000000e+00  -0.00000000e+00  -0.00000000e+00 ...,  -2.62087981e+01\n",
      "   -0.00000000e+00  -0.00000000e+00]\n",
      " ..., \n",
      " [ -4.29758438e+02  -0.00000000e+00  -0.00000000e+00 ...,  -0.00000000e+00\n",
      "   -0.00000000e+00  -0.00000000e+00]\n",
      " [ -0.00000000e+00  -0.00000000e+00  -0.00000000e+00 ...,  -0.00000000e+00\n",
      "   -0.00000000e+00  -0.00000000e+00]\n",
      " [ -4.29758438e+02  -0.00000000e+00  -0.00000000e+00 ...,  -0.00000000e+00\n",
      "   -0.00000000e+00  -0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# hidden_layer_input = np.dot(data[\"x_train\"], wh) + bh\n",
    "# hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "# output_layer_input = np.dot(hidden_layer_output, wo) + bo\n",
    "# output_layer_output = softmax(output_layer_input)\n",
    "# error_output = log_loss(output_layer_output, data[\"y_train\"])\n",
    "\n",
    "# print(hidden_layer_input.shape, hidden_layer_output.shape, output_layer_input.shape, output_layer_output.shape, error_output.shape)\n",
    "# print(error_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[[  7.22104758e-01   1.21599898e-02   3.64926004e-05   5.16692207e-07\n",
      "    2.34719011e-01   3.01892863e-02   6.58329379e-04   2.41881067e-11\n",
      "    3.21738840e-05   9.94423466e-05]\n",
      " [  7.22104758e-01   1.21599898e-02   3.64926004e-05   5.16692207e-07\n",
      "    2.34719011e-01   3.01892863e-02   6.58329379e-04   2.41881067e-11\n",
      "    3.21738840e-05   9.94423466e-05]\n",
      " [  7.22104758e-01   1.21599898e-02   3.64926004e-05   5.16692207e-07\n",
      "    2.34719011e-01   3.01892863e-02   6.58329379e-04   2.41881067e-11\n",
      "    3.21738840e-05   9.94423466e-05]\n",
      " [  7.22104758e-01   1.21599898e-02   3.64926004e-05   5.16692207e-07\n",
      "    2.34719011e-01   3.01892863e-02   6.58329379e-04   2.41881067e-11\n",
      "    3.21738840e-05   9.94423466e-05]\n",
      " [  7.22104758e-01   1.21599898e-02   3.64926004e-05   5.16692207e-07\n",
      "    2.34719011e-01   3.01892863e-02   6.58329379e-04   2.41881067e-11\n",
      "    3.21738840e-05   9.94423466e-05]\n",
      " [  7.22104758e-01   1.21599898e-02   3.64926004e-05   5.16692207e-07\n",
      "    2.34719011e-01   3.01892863e-02   6.58329379e-04   2.41881067e-11\n",
      "    3.21738840e-05   9.94423466e-05]\n",
      " [  7.22104758e-01   1.21599898e-02   3.64926004e-05   5.16692207e-07\n",
      "    2.34719011e-01   3.01892863e-02   6.58329379e-04   2.41881067e-11\n",
      "    3.21738840e-05   9.94423466e-05]\n",
      " [  7.22104758e-01   1.21599898e-02   3.64926004e-05   5.16692207e-07\n",
      "    2.34719011e-01   3.01892863e-02   6.58329379e-04   2.41881067e-11\n",
      "    3.21738840e-05   9.94423466e-05]\n",
      " [  7.22104758e-01   1.21599898e-02   3.64926004e-05   5.16692207e-07\n",
      "    2.34719011e-01   3.01892863e-02   6.58329379e-04   2.41881067e-11\n",
      "    3.21738840e-05   9.94423466e-05]\n",
      " [  7.22104758e-01   1.21599898e-02   3.64926004e-05   5.16692207e-07\n",
      "    2.34719011e-01   3.01892863e-02   6.58329379e-04   2.41881067e-11\n",
      "    3.21738840e-05   9.94423466e-05]]\n",
      "[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 1, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(output_layer_output[0]))\n",
    "print(output_layer_output[:10])\n",
    "print(data[\"y_train\"][:10])\n",
    "# print(data[\"y_train\"][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = -0.01\n",
    "\n",
    "# d_loss_d_output = d_softmax(output_layer_output, data[\"y_train\"])\n",
    "# d_loss_d_wo = np.dot(d_loss_d_output.T,  hidden_layer_output)\n",
    "\n",
    "# d_loss_d_hidden_output = np.dot(wo, d_loss_d_output.T)\n",
    "\n",
    "# print(d_loss_d_output.shape, d_loss_d_wo.shape, d_loss_d_hidden_output.shape)\n",
    "\n",
    "\n",
    "# slope_output_layer = d_softmax(output_layer_output, data[\"y_train\"])\n",
    "# slope_hidden_layer = d_sigmoid(hidden_layer_output)\n",
    "# d_output =  slope_output_layer * error_output * learning_rate\n",
    "\n",
    "# error_hidden = np.dot(d_output, wo.T)\n",
    "# d_hidden = error_hidden * slope_hidden_layer\n",
    "\n",
    "# wo = wo + np.dot(hidden_layer_output.T, d_output) * learning_rate\n",
    "# wh = wh + np.dot(data[\"x_train\"].T, d_hidden) * learning_rate\n",
    "# bo = bo + np.sum(d_output, axis=0) * learning_rate\n",
    "# bh = bh + np.sum(d_hidden, axis=0) * learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-ae5c2ef4f72b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0mneural_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOutputLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m \u001b[0mneural_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-ae5c2ef4f72b>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x_train, y_train)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mx_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/School/COMP/COMP_551/MLKaggle/MLKaggleComp/venv/lib/python3.5/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mappend\u001b[0;34m(arr, values, axis)\u001b[0m\n\u001b[1;32m   5164\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5165\u001b[0m         \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5166\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_rows, input_cols, learning_rate=0.01, num_nodes=200, activation_func=\"sigmoid\"):\n",
    "        self.input_rows = input_rows\n",
    "        self.input_cols = input_cols\n",
    "        self.num_nodes = num_nodes\n",
    "        self.next = None\n",
    "        self.prev = None\n",
    "        self.w = np.random.uniform(size=(self.input_cols,num_nodes))\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if activation_func == \"sigmoid\":\n",
    "            self.activation_func = self.sigmoid\n",
    "            self.d_activation_func = self.d_sigmoid\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    def feedforward(self, x):\n",
    "        \"\"\"\n",
    "        return the predictions (represented by a probability)\n",
    "        \"\"\"\n",
    "        # calculate stuff\n",
    "        before_activation = np.dot(x, self.w)\n",
    "        self.output = self.activation_func(before_activation) \n",
    "        self.derivative = self.d_activation_func(before_activation)\n",
    "        \n",
    "        \n",
    "        # if there's a next layer\n",
    "        if self.next:\n",
    "            # add bias to the end of each row of self.output\n",
    "            try:\n",
    "                self.output = np.append(self.output, np.ones((self.output.shape[0], 1)), axis=-1)\n",
    "            except ValueError:\n",
    "                self.output = np.append(self.output, 1) \n",
    "            \n",
    "            # call next layer's feedforward step\n",
    "            self.next.feedforward(self.output)\n",
    "\n",
    "        \n",
    "    def backprop(self, prev_deltas):\n",
    "        \"\"\"\n",
    "        compute derivatives and adjust w\n",
    "        \"\"\" \n",
    "        deltas = self.derivative * prev_deltas \n",
    "        if self.prev:\n",
    "            self.prev.backprop(np.dot(self.w[:-1], deltas))\n",
    "        self.w = self.w - (self.learning_rate * self.output[:-1] * deltas)\n",
    "    \n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        sigmoid function\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def d_sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        derivative of sigmoid\n",
    "        \"\"\"\n",
    "        return x * (1. - x)\n",
    "   \n",
    "    \n",
    "class OutputLayer(Layer):\n",
    "    def __init__(self, input_rows, input_cols, learning_rate=0.01, num_nodes=200, activation_func=\"softmax\"):\n",
    "        self.input_rows = input_rows\n",
    "        self.input_cols = input_cols\n",
    "        self.num_nodes = num_nodes\n",
    "        self.next = None\n",
    "        self.prev = None\n",
    "        self.w = np.random.uniform(size=(self.input_cols,num_nodes))\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if activation_func == \"softmax\":\n",
    "            self.activation_func = self.softmax\n",
    "            self.backprop_func = lambda x, target: x - target\n",
    "            \n",
    "        elif activation_func == \"sigmoid\":\n",
    "            self.activation_func = self.sigmoid\n",
    "            self.backprop_func = lambda x, target: self.d_sigmoid(x) * (x - target)\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # Value is unused.\n",
    "        self.d_activation_func = lambda x: None\n",
    "     \n",
    "    def backprop(self, targets):\n",
    "        deltas = self.backprop_func(self.output, targets)\n",
    "        self.prev.backprop(np.dot(self.w[:-1], deltas))\n",
    "        self.w = self.w - self.learning_rate * np.dot(self.output, deltas)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        e = np.exp(x - np.amax(x))\n",
    "        dist = e / np.sum(e)\n",
    "        return dist\n",
    "        # e = np.exp(x)\n",
    "        # e_sum = np.sum(e, axis=1)\n",
    "        # y = []\n",
    "        # for i in range(len(e)):\n",
    "        #     y.append(e[i]/e_sum[i])\n",
    "        # return np.array(y)\n",
    "        \n",
    "    \n",
    "class NeuralNet:\n",
    "    def __init__(self, learning_rate, num_epochs):\n",
    "        self.first = None\n",
    "        self.last = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "    def add_layer(self, layer):\n",
    "        \"\"\"\n",
    "        Add layer to the end\n",
    "        \"\"\"\n",
    "        if not self.first:\n",
    "            self.first = layer\n",
    "            self.last = layer\n",
    "            \n",
    "        else:\n",
    "            temp = self.last\n",
    "            temp.next = layer\n",
    "            layer.prev = temp\n",
    "            self.last = layer\n",
    "            \n",
    "    def fit(self, x_train, y_train):\n",
    "        x_input = np.append(x_train, np.ones((x_train.shape[0], 1)), axis=-1)\n",
    "        \n",
    "        for i in range(self.num_epochs): \n",
    "            for j in range(len(x_input)):\n",
    "                self.first.feedforward(x_input[j])\n",
    "                self.last.backprop(y_train[j])\n",
    "                \n",
    "    def predict(self, x):\n",
    "        x_input = np.append(x, np.ones((x.shape[0], 1)), axis=-1)\n",
    "        first.feedforward(x_input)\n",
    "        return self.last.output\n",
    "\n",
    "\n",
    "x_tr = data[\"x_train\"]\n",
    "y_tr = data[\"y_train\"]\n",
    "\n",
    "neural_net = NeuralNet(0.1, 10)\n",
    "\n",
    "neural_net.add_layer(Layer(x_tr.shape[0], x_tr.shape[1] + 1, 0.1, 300))\n",
    "neural_net.add_layer(Layer(x_tr.shape[0], 300 + 1, 0.1, 200))\n",
    "neural_net.add_layer(OutputLayer(x_tr.shape[0], 200 + 1, 0.1, 10))\n",
    "\n",
    "neural_net.fit(x_tr, y_tr)\n",
    "print(\"Done\")\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error at i = 0 -5440394.06185\n",
      "[[  94.40669398  104.89006279   90.80468338  106.35614204   95.30846331\n",
      "   113.42472528   99.04346378   98.6508905    96.01620947   99.3465586 ]\n",
      " [  94.40669398  104.89006279   90.80468338  106.35614204   95.30846331\n",
      "   113.42472528   99.04346378   98.6508905    96.01620947   99.3465586 ]\n",
      " [  94.40669398  104.89006279   90.80468338  106.35614204   95.30846331\n",
      "   113.42472528   99.04346378   98.6508905    96.01620947   99.3465586 ]]\n",
      "[[  5.49690578e-09   1.96330386e-04   1.49894247e-10   8.50545684e-04\n",
      "    1.35441495e-08   9.98951359e-01   5.67337947e-07   3.83132879e-07\n",
      "    2.74866612e-08   7.68199888e-07]\n",
      " [  5.49690578e-09   1.96330386e-04   1.49894247e-10   8.50545684e-04\n",
      "    1.35441495e-08   9.98951359e-01   5.67337947e-07   3.83132879e-07\n",
      "    2.74866612e-08   7.68199888e-07]\n",
      " [  5.49690578e-09   1.96330386e-04   1.49894247e-10   8.50545684e-04\n",
      "    1.35441495e-08   9.98951359e-01   5.67337947e-07   3.83132879e-07\n",
      "    2.74866612e-08   7.68199888e-07]]\n",
      "error at i = 10 -924870.91148\n",
      "[[ 113.12732948  112.69396486  113.10811974  112.89870506  113.08751214\n",
      "   112.88006273  113.10175337  113.12530813  113.15966164  113.06465864]\n",
      " [ 113.12732948  112.69396486  113.10811974  112.89870506  113.08751214\n",
      "   112.88006273  113.10175337  113.12530813  113.15966164  113.06465864]\n",
      " [ 113.12732948  112.69396486  113.10811974  112.89870506  113.08751214\n",
      "   112.88006273  113.10175337  113.12530813  113.15966164  113.06465864]]\n",
      "[[ 0.10974943  0.0711532   0.1076613   0.08731965  0.10546536  0.08570688\n",
      "   0.10697806  0.10952782  0.11335586  0.10308244]\n",
      " [ 0.10974943  0.0711532   0.1076613   0.08731965  0.10546536  0.08570688\n",
      "   0.10697806  0.10952782  0.11335586  0.10308244]\n",
      " [ 0.10974943  0.0711532   0.1076613   0.08731965  0.10546536  0.08570688\n",
      "   0.10697806  0.10952782  0.11335586  0.10308244]]\n",
      "error at i = 20 -921680.415031\n",
      "[[ 113.18042494  113.02867115  113.16807114  113.1536146   113.15511921\n",
      "   113.08456835  113.18892818  113.23748586  113.21154818  113.15369847]\n",
      " [ 113.18042494  113.02867115  113.16807114  113.1536146   113.15511921\n",
      "   113.08456835  113.18892818  113.23748586  113.21154818  113.15369847]\n",
      " [ 113.18042494  113.02867115  113.16807114  113.1536146   113.15511921\n",
      "   113.08456835  113.18892818  113.23748586  113.21154818  113.15369847]]\n",
      "[[ 0.10228532  0.08788353  0.10102948  0.09957945  0.09972939  0.09293586\n",
      "   0.10315879  0.10829155  0.10551883  0.0995878 ]\n",
      " [ 0.10228532  0.08788353  0.10102948  0.09957945  0.09972939  0.09293586\n",
      "   0.10315879  0.10829155  0.10551883  0.0995878 ]\n",
      " [ 0.10228532  0.08788353  0.10102948  0.09957945  0.09972939  0.09293586\n",
      "   0.10315879  0.10829155  0.10551883  0.0995878 ]]\n",
      "error at i = 30 -921284.647466\n",
      "[[ 113.19933606  113.16287128  113.18951215  113.20990457  113.17900025\n",
      "   113.12334824  113.21194117  113.26143875  113.22568633  113.18028587]\n",
      " [ 113.19933606  113.16287128  113.18951215  113.20990457  113.17900025\n",
      "   113.12334824  113.21194117  113.26143875  113.22568633  113.18028587]\n",
      " [ 113.19933606  113.16287128  113.18951215  113.20990457  113.17900025\n",
      "   113.12334824  113.21194117  113.26143875  113.22568633  113.18028587]]\n",
      "[[ 0.10043867  0.09684216  0.0994568   0.10150578  0.09841679  0.09308932\n",
      "   0.10171272  0.10687393  0.10312043  0.0985434 ]\n",
      " [ 0.10043867  0.09684216  0.0994568   0.10150578  0.09841679  0.09308932\n",
      "   0.10171272  0.10687393  0.10312043  0.0985434 ]\n",
      " [ 0.10043867  0.09684216  0.0994568   0.10150578  0.09841679  0.09308932\n",
      "   0.10171272  0.10687393  0.10312043  0.0985434 ]]\n",
      "error at i = 40 -921252.775686\n",
      "[[ 113.20598236  113.23148344  113.19703192  113.22320619  113.18730637\n",
      "   113.13448792  113.21862741  113.26554165  113.22983774  113.18890017]\n",
      " [ 113.20598236  113.23148344  113.19703192  113.22320619  113.18730637\n",
      "   113.13448792  113.21862741  113.26554165  113.22983774  113.18890017]\n",
      " [ 113.20598236  113.23148344  113.19703192  113.22320619  113.18730637\n",
      "   113.13448792  113.21862741  113.26554165  113.22983774  113.18890017]]\n",
      "[[ 0.09971989  0.10229556  0.09883134  0.10145233  0.09787481  0.09283936\n",
      "   0.10098886  0.10583957  0.10212735  0.09803092]\n",
      " [ 0.09971989  0.10229556  0.09883134  0.10145233  0.09787481  0.09283936\n",
      "   0.10098886  0.10583957  0.10212735  0.09803092]\n",
      " [ 0.09971989  0.10229556  0.09883134  0.10145233  0.09787481  0.09283936\n",
      "   0.10098886  0.10583957  0.10212735  0.09803092]]\n",
      "error at i = 50 -921298.827528\n",
      "[[ 113.20761607  113.26996708  113.19902405  113.22570528  113.18962088\n",
      "   113.13788343  113.21991984  113.26448688  113.23025826  113.19122421]\n",
      " [ 113.20761607  113.26996708  113.19902405  113.22570528  113.18962088\n",
      "   113.13788343  113.21991984  113.26448688  113.23025826  113.19122421]\n",
      " [ 113.20761607  113.26996708  113.19902405  113.22570528  113.18962088\n",
      "   113.13788343  113.21991984  113.26448688  113.23025826  113.19122421]]\n",
      "[[ 0.09934053  0.10573169  0.09849065  0.10115388  0.09756887  0.09264927\n",
      "   0.10057035  0.10515385  0.10161548  0.09772543]\n",
      " [ 0.09934053  0.10573169  0.09849065  0.10115388  0.09756887  0.09264927\n",
      "   0.10057035  0.10515385  0.10161548  0.09772543]\n",
      " [ 0.09934053  0.10573169  0.09849065  0.10115388  0.09756887  0.09264927\n",
      "   0.10057035  0.10515385  0.10161548  0.09772543]]\n"
     ]
    }
   ],
   "source": [
    "def train_mlp(data):\n",
    "    # set number of hidden nodes\n",
    "    num_hidden_nodes = 200\n",
    "    num_input = 40000\n",
    "    input_size = 4096\n",
    "    learning_rate = 0.001\n",
    "    num_epoch = 51\n",
    "\n",
    "    # initialize random wh, bh, wo, and bo\n",
    "    wh = np.random.uniform(size=(input_size,num_hidden_nodes))\n",
    "    bh = np.random.uniform(size=(1, num_hidden_nodes))\n",
    "    wo = np.random.uniform(size=(num_hidden_nodes, 10))\n",
    "    bo = np.random.uniform(size=(1, 10))\n",
    "\n",
    "    \n",
    "    for i in range(num_epoch):\n",
    "        hidden_layer_input = np.dot(data[\"x_train\"], wh) + bh\n",
    "        hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "        output_layer_input = np.dot(hidden_layer_output, wo) + bo\n",
    "        output_layer_output = softmax(output_layer_input)\n",
    "        error_output = log_loss(output_layer_output)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(\"error at i =\", i,  np.sum(error_output))\n",
    "            print(output_layer_input[:3])\n",
    "            print(output_layer_output[:3])\n",
    "#             print(wo[0], wh[0], bo[0], bh[0])\n",
    "\n",
    "\n",
    "        slope_output_layer = d_softmax(output_layer_output, data[\"y_train\"])\n",
    "        slope_hidden_layer = d_sigmoid(hidden_layer_output)\n",
    "        d_output = error_output * slope_output_layer * learning_rate\n",
    "\n",
    "        error_hidden = np.dot(d_output, wo.T)\n",
    "        d_hidden = error_hidden * slope_hidden_layer\n",
    "\n",
    "        wo = wo + np.dot(hidden_layer_output.T, d_output) * learning_rate\n",
    "        wh = wh + np.dot(data[\"x_train\"].T, d_hidden) * learning_rate\n",
    "        bo = bo + np.sum(d_output, axis=0) * learning_rate\n",
    "        bh = bh + np.sum(d_hidden, axis=0) * learning_rate\n",
    "        \n",
    "    model = {\"wh\": wh, \"bh\": bh, \"wo\": wo, \"bo\": bo}\n",
    "        \n",
    "    return model\n",
    "\n",
    "model = train_mlp(data)\n",
    "\n",
    "# def predict(x, model, y):\n",
    "#     hidden_input = np.dot(x, model[\"wh\"]) + model[\"bh\"]\n",
    "#     hidden_output = sigmoid(hidden_input)\n",
    "#     outer_input = np.dot(hidden_output, model['wo']) + model['bo']\n",
    "#     outer_ouput = sigmoid(outer_input)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
