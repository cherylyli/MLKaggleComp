{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import sklearn \n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "x_train = np.loadtxt(\"data/train_x.csv\", delimiter=\",\")\n",
    "y_train = np.loadtxt(\"data/train_y.csv\", delimiter=\",\")\n",
    "x_test = np.loadtxt(\"data/test_x.csv\", delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode y_train data in one-hot encoding\n",
    "def to_one_hot(y):\n",
    "    y_train_one_hot = [[0 for i in range(10)] for i in range(len(y))]\n",
    "    for i in range(len(y)):\n",
    "        y_train_one_hot[i][int(y[i])] = 1\n",
    "\n",
    "    return np.array(y_train_one_hot)\n",
    "\n",
    "def from_one_hot(one_hot_data):\n",
    "    y = []\n",
    "    for row in one_hot_data:\n",
    "        y.append(np.argmax(row))\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(img):\n",
    "    plt.close()\n",
    "    plt.imshow(img, cmap='gray_r')\n",
    "    plt.show()\n",
    "\n",
    "x_train_reshaped = x_train.reshape(-1, 64, 64)\n",
    "y_train_reshaped = y_train.reshape(-1, 1) \n",
    "x_test_reshaped = x_test.reshape(-1, 64, 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(x):\n",
    "    \"\"\"\n",
    "    Because the only thing that matters is the numbers in the picture, which are black, \n",
    "    we recode the pixels as 1 if the pixel was 255 and -1 otherwise. To reduce noise and to\n",
    "    reduce overflow/underflow in later stages.\n",
    "    \"\"\"\n",
    "    new_data = []\n",
    "    for row in x:\n",
    "        new_row = []\n",
    "        for pixel in row:\n",
    "            if pixel == 255:\n",
    "                new_row.append(1.0)\n",
    "            else:\n",
    "                new_row.append(-1.0)\n",
    "        new_data.append(new_row)\n",
    "    return np.array(new_data)\n",
    "\n",
    "x_train = clean_data(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training data into train / valid sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train_s, x_valid_s, y_train_s, y_valid_s = train_test_split(x_train, y_train, train_size=0.8, test_size=0.2)\n",
    "data = {\n",
    "    \"x_train\": x_train_s,\n",
    "    \"x_valid\": x_valid_s,\n",
    "    \"y_train\": to_one_hot(y_train_s),\n",
    "    \"y_valid\": to_one_hot(y_valid_s),\n",
    "    \"y_train_og\": y_train_s,\n",
    "    \"y_valid_og\": y_valid_s,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Linear Classifier: Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run CNN on training data with the validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_linear_svm(data):\n",
    "    \"\"\"\n",
    "    Using out-of-the-box linear SVM to classify data\n",
    "    \"\"\"\n",
    "    clf = LinearSVC()\n",
    "    \n",
    "    y_pred = clf.fit(data[\"x_train\"], data[\"y_train\"]).predict(data[\"x_valid\"])\n",
    "    print(y_pred)\n",
    "    return metrics.accuracy_score(data[\"y_valid\"], y_pred, average=\"macro\"), y_pred\n",
    "    \n",
    "# score, y_pred = baseline_linear_svm(data)\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Represents a layer in the neural net.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_rows, input_cols, learning_rate=0.01, num_nodes=200, activation_func=\"sigmoid\"):\n",
    "        self.input_rows = input_rows\n",
    "        self.input_cols = input_cols\n",
    "        self.num_nodes = num_nodes\n",
    "        self.next = None\n",
    "        self.prev = None\n",
    "        input_range = 1.0 / input_rows ** (1/2)\n",
    "        self.w = np.random.normal(loc=0, scale=input_range, size=(self.input_cols,num_nodes))\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if activation_func == \"sigmoid\":\n",
    "            self.activation_func = expit\n",
    "            self.d_activation_func = lambda x: x * (1. - x)\n",
    "            \n",
    "        elif activation_func == \"tanh\":\n",
    "            self.activation_func = lambda x: np.tanh(x)\n",
    "            self.d_activation_func = lambda x: 1 - np.square(x)\n",
    "            \n",
    "        elif activation_func == \"relu\":\n",
    "            self.activation_func = lambda x: np.maximum(0,x)\n",
    "            self.d_activation_func = lambda x: x/x\n",
    "        else:\n",
    "            raise Error\n",
    "        \n",
    "    def feedforward(self, x):\n",
    "        \"\"\"\n",
    "        return the predictions (represented by a probability)\n",
    "        \"\"\"\n",
    "        # calculate stuff\n",
    "        self.input = x\n",
    "        before_activation = np.dot(x, self.w)\n",
    "        self.output = self.activation_func(before_activation) \n",
    "        self.derivative = self.d_activation_func(before_activation)\n",
    "        \n",
    "        # if there's a next layer\n",
    "        if self.next:\n",
    "            passed_output = []\n",
    "            # add bias to the end of each row of self.output\n",
    "            # need to check if the input was a vector or a matrix\n",
    "            try:\n",
    "                passed_output = np.append(self.output, np.ones((self.output.shape[0], 1)), axis=-1)\n",
    "            except ValueError:\n",
    "                passed_output = np.append(self.output, 1) \n",
    "            \n",
    "            # call next layer's feedforward step\n",
    "            self.next.feedforward(passed_output)\n",
    " \n",
    "    def backprop(self, prev_deltas):\n",
    "        \"\"\"\n",
    "        compute derivatives and adjust w\n",
    "        \"\"\" \n",
    "        deltas = prev_deltas * self.derivative\n",
    "        if self.prev:\n",
    "            self.prev.backprop(np.dot(self.w[:-1], deltas.T).T)\n",
    "        self.w = self.w - (self.learning_rate * np.dot(self.input.T, deltas))\n",
    "\n",
    "\n",
    "class OutputLayer(Layer):\n",
    "    \"\"\"\n",
    "    Similar to the Layer class, but designed to handle the start of the backprop algorithm.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_rows, input_cols, learning_rate=0.01, num_nodes=200, activation_func=\"softmax\"):\n",
    "        self.input_rows = input_rows\n",
    "        self.input_cols = input_cols\n",
    "        self.num_nodes = num_nodes\n",
    "        self.next = None\n",
    "        self.prev = None\n",
    "        input_range = 1.0 / input_rows ** (1/2)\n",
    "        self.w = np.random.normal(loc=0, scale=input_range, size=(self.input_cols,num_nodes))\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if activation_func == \"softmax\":\n",
    "            self.activation_func = lambda x: 1 / (1 + np.exp(-x))\n",
    "            self.backprop_func = lambda x, target: x - target\n",
    "            \n",
    "        elif activation_func == \"sigmoid\":\n",
    "            self.activation_func = self.sigmoid\n",
    "            self.backprop_func = lambda x, target: self.d_sigmoid(x) * (x - target)\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    def backprop(self, targets):\n",
    "        deltas = self.backprop_func(self.output, targets)\n",
    "        self.prev.backprop(np.dot(self.w[:-1], deltas.T).T)\n",
    "        self.w = self.w - self.learning_rate * np.dot(self.input.T, deltas)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        y = []\n",
    "        for i in range(len(x)):\n",
    "            row_exp = np.exp(x[i] - np.amax(x[i]))\n",
    "            row_sum = np.sum(row_exp)\n",
    "            y.append(row_exp/row_sum)\n",
    "        return np.array(y)\n",
    "        \n",
    "    \n",
    "class NeuralNet:\n",
    "    \"\"\"\n",
    "    Stores a linked list of layers, and starts the method calls.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate, num_epochs):\n",
    "        self.first = None\n",
    "        self.last = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "    def add_layer(self, layer):\n",
    "        \"\"\"\n",
    "        Add layer to the end\n",
    "        \"\"\"\n",
    "        if not self.first:\n",
    "            self.first = layer\n",
    "            self.last = layer\n",
    "            \n",
    "        else:\n",
    "            temp = self.last\n",
    "            temp.next = layer\n",
    "            layer.prev = temp\n",
    "            self.last = layer\n",
    "            \n",
    "    def fit(self, x_train, y_train, x_valid, y_valid):\n",
    "        print(x_valid.shape, y_valid.shape)\n",
    "        x_input = np.append(x_train, np.ones((x_train.shape[0], 1)), axis=-1)\n",
    "        init_acc = self.get_accuracy(x_valid, y_valid)\n",
    "        \n",
    "        for i in range(self.num_epochs):\n",
    "\n",
    "            for j in range(0, len(x_input), 1000):\n",
    "                self.first.feedforward(x_input[j:j+1000])\n",
    "                self.last.backprop(y_train[j:j+1000])\n",
    "                \n",
    "        final_acc = self.get_accuracy(x_valid, y_valid)\n",
    "        print(\"Initial accuracy, final accuracy\", init_acc, final_acc)\n",
    "        return final_acc\n",
    "                \n",
    "    def predict(self, x):\n",
    "        x_input = np.append(x, np.ones((x.shape[0], 1)), axis=-1)\n",
    "        self.first.feedforward(x_input)\n",
    "        return self.last.output\n",
    "    \n",
    "    def get_accuracy(self, x_valid, y_valid):\n",
    "        y_pred = self.predict(x_valid)\n",
    "        return metrics.accuracy_score(from_one_hot(y_valid), from_one_hot(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr = data[\"x_train\"]\n",
    "y_tr = data[\"y_train\"]\n",
    "\n",
    "\n",
    "# neural_net = NeuralNet(1e-5, 2)\n",
    "# # neural_net.k_fold(x_train, y_train)\n",
    "# neural_net.add_layer(Layer(x_tr.shape[0], x_tr.shape[1] + 1, 1e-5, 3500, activation_func=\"relu\"))\n",
    "# # neural_net.add_layer(Layer(x_tr.shape[0], 300 + 1, 1e-6, 200))\n",
    "# neural_net.add_layer(OutputLayer(x_tr.shape[0], 3500 + 1, 1e-5, 10))\n",
    "# # neural_net.fit(x_tr, y_tr, data[\"x_valid\"], data[\"y_valid\"])\n",
    "# # print(\"Done\")\n",
    "        \n",
    "def k_fold(x, y):\n",
    "    # divide into 5 folds\n",
    "    y = to_one_hot(y)\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    kf.get_n_splits(x)\n",
    "    accuracy_scores = []\n",
    "\n",
    "    # for each fold, train and compute accuracy\n",
    "    for train_index, test_index in kf.split(x):\n",
    "\n",
    "        # create 'new' train & valid sets based on kfold split\n",
    "        new_xt, new_yt = [], []\n",
    "        new_xv, new_yv = [], []\n",
    "        for ind in train_index:\n",
    "            new_xt.append(x[ind])\n",
    "            new_yt.append(y[ind])\n",
    "\n",
    "        for ind in test_index:\n",
    "            new_xv.append(x[ind])\n",
    "            new_yv.append(y[ind])\n",
    "        \n",
    "        \n",
    "        new_xt = np.array(new_xt)\n",
    "        new_yt = np.array(new_yt)\n",
    "        new_xv = np.array(new_xv)\n",
    "        new_yv = np.array(new_yv)\n",
    "        print(new_xt.shape, new_yt.shape, new_xv.shape, new_yv.shape)\n",
    "        \n",
    "        nn = NeuralNet(1e-5, 2)\n",
    "        nn.add_layer(Layer(new_xt.shape[0], new_xt.shape[1] + 1, 1e-5, 3000, activation_func=\"relu\"))\n",
    "        nn.add_layer(OutputLayer(new_xt.shape[0], 3000 + 1, 1e-5, 10))\n",
    "        acc_score = nn.fit(new_xt, new_yt, new_xv, new_yv)\n",
    "        accuracy_scores.append(acc_score)\n",
    "\n",
    "    print(accuracy_scores, np.average(accuracy_scores))\n",
    "    return accuracy_scores\n",
    "\n",
    "k_fold(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Neural Net with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These classes are meant to be equivalent to the previous classes.\n",
    "The only difference is that this uses the pytorch Tensor libraries to perform the computation on the GPU.\n",
    "Only functions for matrix computations were used, it served only to do the work of numpy on the GPU.\n",
    "\"\"\"\n",
    "\n",
    "class TorchLayer:\n",
    "    def __init__(self, input_rows, input_cols, learning_rate=0.01, num_nodes=200, activation_func=\"sigmoid\"):\n",
    "        self.input_rows = input_rows\n",
    "        self.input_cols = input_cols\n",
    "        self.num_nodes = num_nodes\n",
    "        self.next = None\n",
    "        self.prev = None\n",
    "        input_range = 1.0 / input_rows ** (1/2)\n",
    "        self.w = torch.from_numpy(np.random.normal(loc=0, scale=input_range, size=(self.input_cols,num_nodes))).cuda()\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if activation_func == \"sigmoid\":\n",
    "            self.activation_func = torch.sigmoid\n",
    "            self.d_activation_func = self.d_sigmoid\n",
    "            \n",
    "            \n",
    "        elif activation_func == \"tanh\":\n",
    "            self.activation_func = self.tanh\n",
    "            self.d_activation_func = self.d_tanh\n",
    "            \n",
    "        elif activation_func == \"relu\":\n",
    "            self.activation_func = self.relu\n",
    "            self.d_activation_func = self.d_relu\n",
    "        else:\n",
    "            raise Error\n",
    "        \n",
    "    def feedforward(self, x):\n",
    "        \"\"\"\n",
    "        return the predictions (represented by a probability)\n",
    "        \"\"\"\n",
    "        # calculate stuff\n",
    "        self.input = x\n",
    "        before_activation = torch.mm(x, self.w)\n",
    "        self.output = self.activation_func(before_activation) \n",
    "        self.derivative = self.d_activation_func(before_activation)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # if there's a next layer\n",
    "        if self.next:\n",
    "            # add bias to the end of each row of the output.\n",
    "            passed_output = torch.cat((self.output, torch.ones(self.output.size()[0], 1).double().cuda()), 1)\n",
    "            \n",
    "            # call next layer's feedforward step\n",
    "            self.next.feedforward(passed_output)\n",
    "\n",
    "    def backprop(self, prev_deltas):\n",
    "        \"\"\"\n",
    "        compute derivatives and adjust w\n",
    "        \"\"\" \n",
    "        deltas = prev_deltas * self.derivative\n",
    "        if self.prev:\n",
    "            self.prev.backprop(torch.mm(self.w[:-1], deltas.t()).t())\n",
    "        self.w = self.w - (self.learning_rate * torch.mm(self.input.t(), deltas))\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return torch.tanh(x)\n",
    "        \n",
    "    def d_tanh(self, x):\n",
    "        return 1 - (torch.tanh(x))**2\n",
    "        \n",
    "    def relu(self, x):\n",
    "        return torch.clamp(x, min=0)\n",
    "    \n",
    "    def d_relu(self, x):\n",
    "        return torch.gt(x, 0).double()\n",
    "    \n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        sigmoid function\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def d_sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        derivative of sigmoid\n",
    "        \"\"\"\n",
    "        return x * (1. - x)\n",
    "   \n",
    "    \n",
    "class TorchOutputLayer(TorchLayer):\n",
    "    def __init__(self, input_rows, input_cols, learning_rate=0.01, num_nodes=200, activation_func=\"softmax\"):\n",
    "        self.input_rows = input_rows\n",
    "        self.input_cols = input_cols\n",
    "        self.num_nodes = num_nodes\n",
    "        self.next = None\n",
    "        self.prev = None\n",
    "        self.w = torch.from_numpy(np.random.uniform(size=(self.input_cols,num_nodes)) / np.sqrt(self.input_cols)).cuda()\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if activation_func == \"softmax\":\n",
    "            self.activation_func = self.softmax\n",
    "            self.backprop_func = lambda x, target: x - target\n",
    "            \n",
    "        elif activation_func == \"sigmoid\":\n",
    "            self.activation_func = self.sigmoid\n",
    "            self.backprop_func = lambda x, target: self.d_sigmoid(x) * (x - target)\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # Value is unused.\n",
    "        self.d_activation_func = lambda x: None\n",
    "     \n",
    "    def backprop(self, targets):\n",
    "        deltas = self.backprop_func(self.output, targets)\n",
    "        self.prev.backprop(torch.mm(self.w[:-1], deltas.t()).t())\n",
    "        self.w = self.w - self.learning_rate * torch.mm(self.input.t(), deltas)\n",
    "        \n",
    "    \n",
    "    def softmax(self, x):\n",
    "        # This is technically from the neural net package, but it's really just a way to compute Softmax.\n",
    "        return torch.nn.functional.softmax(torch.autograd.Variable(x), 1).data\n",
    "        \n",
    "    \n",
    "class TorchNeuralNet:\n",
    "    def __init__(self, learning_rate, num_epochs):\n",
    "        self.first = None\n",
    "        self.last = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "    def add_layer(self, layer):\n",
    "        \"\"\"\n",
    "        Add layer to the end\n",
    "        \"\"\"\n",
    "        if not self.first:\n",
    "            self.first = layer\n",
    "            self.last = layer\n",
    "            \n",
    "        else:\n",
    "            temp = self.last\n",
    "            temp.next = layer\n",
    "            layer.prev = temp\n",
    "            self.last = layer\n",
    "            \n",
    "    def fit(self, x_train, y_train):\n",
    "        x_input = np.append(x_train, np.ones((x_train.shape[0], 1)), axis=-1) \n",
    "        \n",
    "        for i in range(self.num_epochs):\n",
    "\n",
    "            for j in range(0, len(x_input), 50):\n",
    "                self.first.feedforward(torch.from_numpy(x_input[j:j+500]).cuda())\n",
    "                self.last.backprop(torch.from_numpy(y_train[j:j+50]).double().cuda()) \n",
    "                \n",
    "    def predict(self, x):\n",
    "        x_input = torch.from_numpy(np.append(x, np.ones((x.shape[0], 1)), axis=-1)).cuda()\n",
    "        self.first.feedforward(x_input)\n",
    "        return self.last.output.cpu().numpy()\n",
    "    \n",
    "    def get_accuracy(self):\n",
    "        y_pred = np.empty((len(data[\"x_valid\"]), 10), dtype=data[\"x_valid\"].dtype)\n",
    "        for i in range(0, len(data[\"x_valid\"]), 1000):\n",
    "            y_pred[i:i+1000] = self.predict(data[\"x_valid\"][i:i+1000])\n",
    "            \n",
    "        return metrics.accuracy_score(data[\"y_valid_og\"], from_one_hot(y_pred))\n",
    "    \n",
    "    def get_train_accuracy(self):\n",
    "        y_pred = np.empty((len(data[\"x_train\"]), 10), dtype=data[\"x_train\"].dtype)\n",
    "        for i in range(0, len(data[\"x_train\"]), 1000):\n",
    "            y_pred[i:i+1000] = self.predict(data[\"x_train\"][i:i+1000])\n",
    "            \n",
    "        return metrics.accuracy_score(data[\"y_train_og\"], from_one_hot(y_pred))\n",
    "\n",
    "\n",
    "x_tr = data[\"x_train\"]\n",
    "y_tr = data[\"y_train\"]\n",
    "\n",
    "# Run a sample Neural Net.\n",
    "\n",
    "neural_net = TorchNeuralNet(1e-5, 400)\n",
    "\n",
    "neural_net.add_layer(TorchLayer(x_tr.shape[0], x_tr.shape[1] + 1, 1e-4, 25, activation_func=\"tanh\"))\n",
    "neural_net.add_layer(TorchLayer(x_tr.shape[0], 25 + 1, 1e-4, 75, activation_func=\"relu\"))\n",
    "neural_net.add_layer(TorchOutputLayer(x_tr.shape[0], 75 + 1, 1e-4, 10))\n",
    "\n",
    "neural_net.fit(x_tr, y_tr)\n",
    "print(neural_net.get_accuracy())\n",
    "print(\"Done\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Convolution Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Class to hold all the layers of the neural net, and that all calls are made to.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layers = []\n",
    "        self.num_layers = 0\n",
    "    \n",
    "    def add_layer(self, layer):\n",
    "        self.num_layers += 1\n",
    "        self.add_module(str(self.num_layers), layer)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class View_Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Just a way to make the call to the method view() of a Tensor, and use it as part of the neural net.\n",
    "    This is needed before the linear layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, param1, param2):\n",
    "        super(View_Layer, self).__init__()\n",
    "        self.param1 = param1\n",
    "        self.param2 = param2\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.view(self.param1, self.param2)\n",
    "    \n",
    "    \n",
    "def fit(cnn, x_train, y_train, epochs, lr=0.01, momentum=0.9):\n",
    "    \"\"\"\n",
    "    Train the model on the given inputs x_train and y_train, stored in numpy arrays.\n",
    "    \"\"\"\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(cnn.parameters(), lr=lr, momentum=momentum)\n",
    "    \n",
    "    \n",
    "    for i in range(0, epochs):\n",
    "        for j in range(0, x_train.shape[0], 500):\n",
    "            x_in = Variable(torch.from_numpy(x_train[j:j+500]).cuda()).view(-1, 1, 64, 64)\n",
    "            y_in = Variable(torch.from_numpy(y_train[j:j+500]).long().cuda())\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            y_pred = cnn(x_in)\n",
    "            loss = crit(y_pred, y_in)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "def fit_loader(cnn, dataloader, epochs, lr=0.01, momentum=0.9):\n",
    "    \"\"\"\n",
    "    Same as the fit method, but it takes a dataloader for the data instead of numpy arrays. \n",
    "    \"\"\"\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(cnn.parameters(), lr=lr, momentum=momentum)\n",
    "    \n",
    "    for epoch in range(0, epochs):\n",
    "        for i, entries in enumerate(dataloader, 0):\n",
    "            x_in, y_in = entries\n",
    "            x_in = Variable(x_in.cuda())\n",
    "            y_in = Variable(y_in.long().cuda())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            y_pred = cnn(x_in)\n",
    "            loss = crit(y_pred, y_in)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "def predict(cnn, x):\n",
    "    return cnn(Variable(torch.from_numpy(x).cuda().view(-1, 1, 64, 64))).cpu().data.numpy()\n",
    "\n",
    "def predict_batch(cnn, x):\n",
    "    y_pred = np.empty((len(x), 10), dtype=x.dtype)\n",
    "    for i in range(0, len(x), 1000):\n",
    "        y_pred[i:i+1000] = predict(cnn, x[i:i+1000])\n",
    "            \n",
    "    return y_pred\n",
    "\n",
    "def get_accuracy(cnn, x_valid, y_valid):\n",
    "    y_pred = np.empty((len(x_valid), 10), dtype=y_valid.dtype)\n",
    "    for i in range(0, len(x_valid), 100):\n",
    "        y_pred[i:i+100] = predict(cnn, x_valid[i:i+100])\n",
    "            \n",
    "    return metrics.accuracy_score(from_one_hot(y_valid), from_one_hot(y_pred))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "\"\"\"\n",
    "This is used to log outputs to both stdout, and to a log file.\n",
    "This is necessary when running the notebook remotely.\n",
    "It makes it possible to close the browser, and keep the output.\n",
    "\"\"\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "def setup_file_logger(log_file):\n",
    "    hdlr = logging.FileHandler(log_file)\n",
    "    formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "    hdlr.setFormatter(formatter)\n",
    "    logger.addHandler(hdlr) \n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "def log(message):\n",
    "    #outputs to Jupyter console\n",
    "    print('{} {}'.format(datetime.datetime.now(), message))\n",
    "    #outputs to file\n",
    "    logger.info(message)\n",
    "\n",
    "setup_file_logger('out.log')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ImageArrayDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This is a Dataset used to hold the training data in numpy arrays.\n",
    "    It also allows for transformation to the data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, x, y, transform=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        if self.transform:\n",
    "            x_trans = self.transform(self.x[idx])\n",
    "        \n",
    "        return (x_trans, self.y[idx])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run CNN on training data with the validation split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"Testing with 9 conv layers.\")\n",
    "\n",
    "x_tr = data[\"x_train\"].reshape(-1, 1, 64, 64)\n",
    "y_tr = data[\"y_train_og\"]\n",
    "\n",
    "\n",
    "# Add a random rotation to the data.\n",
    "trans = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: np.array([255 if i == 1.0 else 0 for i in x.reshape(-1)]).astype(np.uint8).reshape(64, 64, 1)),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomRotation(40),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.view(1, 64, 64).double().apply_(lambda a: 1.0 if a == 1 else -1.0))\n",
    "])\n",
    "\n",
    "train = ImageArrayDataset(x_tr, y_tr, transform=trans)\n",
    "\n",
    "loader = torch.utils.data.DataLoader(train, batch_size=500, shuffle=True, num_workers=2)\n",
    "\n",
    "# Set the padding for each layer.\n",
    "conv_1_pad = 2\n",
    "conv_2_pad = 2\n",
    "conv_3_pad = 2\n",
    "conv_4_pad = 1\n",
    "conv_5_pad = 1\n",
    "conv_6_pad = 0\n",
    "conv_7_pad = 0\n",
    "conv_8_pad = 0\n",
    "conv_9_pad = 0\n",
    "\n",
    "# Set the stride for each layer.\n",
    "conv_1_stride = 1\n",
    "conv_2_stride = 1\n",
    "conv_3_stride = 1\n",
    "conv_4_stride = 1\n",
    "conv_5_stride = 1\n",
    "conv_6_stride = 1\n",
    "conv_7_stride = 1\n",
    "conv_8_stride = 1\n",
    "conv_9_stride = 1\n",
    "\n",
    "# Set the filter size for each layer.\n",
    "conv_1_kernel = 3\n",
    "conv_2_kernel = 3\n",
    "conv_3_kernel = 3\n",
    "conv_4_kernel = 3\n",
    "conv_5_kernel = 3\n",
    "conv_6_kernel = 3\n",
    "conv_7_kernel = 3\n",
    "conv_8_kernel = 3\n",
    "conv_9_kernel = 3\n",
    "\n",
    "# Set the probability of data being dropped before each convolution layer.\n",
    "conv_2_drop = 0.1\n",
    "conv_3_drop = 0.1\n",
    "conv_4_drop = 0.1\n",
    "conv_5_drop = 0.1\n",
    "conv_6_drop = 0.1\n",
    "conv_7_drop = 0.1\n",
    "conv_8_drop = 0.1\n",
    "conv_9_drop = 0.1\n",
    "\n",
    "# Set the number of output channels for each convolution layer.\n",
    "conv_1_out = 25\n",
    "conv_2_out = 35\n",
    "conv_3_out = 50\n",
    "conv_4_out = 65\n",
    "conv_5_out = 80\n",
    "conv_6_out = 90\n",
    "conv_7_out = 100\n",
    "conv_8_out = 110\n",
    "conv_9_out = 120\n",
    "\n",
    "# Set the probability of dropping data before each linear layer.\n",
    "drop_1_p = 0.5\n",
    "drop_2_p = 0.5\n",
    "drop_3_p = 0.5\n",
    "\n",
    "\n",
    "# Initialize CNN.\n",
    "cnn = CNN()\n",
    "\n",
    "# Add batch normalization, and then add the convolution layer.\n",
    "cnn.add_layer(nn.BatchNorm2d(1))\n",
    "cnn.add_layer(nn.Conv2d(1, conv_1_out, conv_1_kernel, padding=conv_1_pad, stride=conv_1_stride))\n",
    "\n",
    "# Track the adjustment to the side length of the output image.\n",
    "side_len = int((64 - conv_1_kernel + (2 * conv_1_pad)) / conv_1_stride) + 1\n",
    "\n",
    "# Use ReLU for activation, and then max pool.\n",
    "cnn.add_layer(nn.ReLU())\n",
    "cnn.add_layer(nn.MaxPool2d(2,2, padding=1))\n",
    "side_len = int((side_len + 2) / 2)\n",
    "\n",
    "\n",
    "# Repeat for each convolution layer.\n",
    "\n",
    "cnn.add_layer(nn.BatchNorm2d(conv_1_out))\n",
    "cnn.add_layer(nn.Dropout2d(conv_2_drop))\n",
    "cnn.add_layer(nn.Conv2d(conv_1_out, conv_2_out, conv_2_kernel, padding=conv_2_pad, stride=conv_2_stride))\n",
    "side_len = int((side_len - conv_2_kernel + (2 * conv_2_pad)) / conv_2_stride) + 1\n",
    "\n",
    "cnn.add_layer(nn.ReLU())\n",
    "cnn.add_layer(nn.MaxPool2d(2, 2, padding=1))\n",
    "side_len = int((side_len + 2) / 2)\n",
    "\n",
    "\n",
    "cnn.add_layer(nn.BatchNorm2d(conv_2_out))\n",
    "cnn.add_layer(nn.Dropout2d(conv_3_drop))\n",
    "cnn.add_layer(nn.Conv2d(conv_2_out, conv_3_out, conv_3_kernel, padding=conv_3_pad, stride=conv_3_stride))\n",
    "side_len = int((side_len - conv_3_kernel + (2 * conv_3_pad)) / conv_3_stride) + 1\n",
    "\n",
    "cnn.add_layer(nn.ReLU())\n",
    "cnn.add_layer(nn.MaxPool2d(2, 2, padding=1))\n",
    "side_len = int((side_len + 2) / 2)\n",
    "\n",
    "\n",
    "cnn.add_layer(nn.BatchNorm2d(conv_3_out))\n",
    "cnn.add_layer(nn.Dropout2d(conv_4_drop))\n",
    "cnn.add_layer(nn.Conv2d(conv_3_out, conv_4_out, conv_4_kernel, padding=conv_4_pad, stride=conv_4_stride))\n",
    "side_len = int((side_len - conv_4_kernel + (2 * conv_4_pad)) / conv_4_stride) + 1\n",
    "\n",
    "# Leave the option open for more Max Pooling.\n",
    "cnn.add_layer(nn.ReLU())\n",
    "# cnn.add_layer(nn.MaxPool2d(2, 2))\n",
    "# side_len = int(side_len / 2)\n",
    "\n",
    "\n",
    "cnn.add_layer(nn.BatchNorm2d(conv_4_out))\n",
    "cnn.add_layer(nn.Dropout2d(conv_5_drop))\n",
    "cnn.add_layer(nn.Conv2d(conv_4_out, conv_5_out, conv_5_kernel, padding=conv_5_pad, stride=conv_5_stride))\n",
    "side_len = int((side_len - conv_5_kernel + (2 * conv_5_pad)) / conv_5_stride) + 1\n",
    "\n",
    "cnn.add_layer(nn.ReLU())\n",
    "# cnn.add_layer(nn.MaxPool2d(2, 2))\n",
    "# side_len = int(side_len / 2)\n",
    "\n",
    "\n",
    "cnn.add_layer(nn.BatchNorm2d(conv_5_out))\n",
    "cnn.add_layer(nn.Dropout2d(conv_6_drop))\n",
    "cnn.add_layer(nn.Conv2d(conv_5_out, conv_6_out, conv_6_kernel, padding=conv_6_pad, stride=conv_6_stride))\n",
    "side_len = int((side_len - conv_6_kernel + (2 * conv_6_pad)) / conv_6_stride) + 1\n",
    "\n",
    "cnn.add_layer(nn.ReLU())\n",
    "# cnn.add_layer(nn.MaxPool2d(2, 2))\n",
    "# side_len = int(side_len / 2)\n",
    "\n",
    "\n",
    "cnn.add_layer(nn.BatchNorm2d(conv_6_out))\n",
    "cnn.add_layer(nn.Dropout2d(conv_7_drop))\n",
    "cnn.add_layer(nn.Conv2d(conv_6_out, conv_7_out, conv_7_kernel, padding=conv_7_pad, stride=conv_7_stride))\n",
    "side_len = int((side_len - conv_7_kernel + (2 * conv_7_pad)) / conv_7_stride) + 1\n",
    "\n",
    "cnn.add_layer(nn.ReLU())\n",
    "# cnn.add_layer(nn.MaxPool2d(2, 2))\n",
    "# side_len = int(side_len / 2)\n",
    "\n",
    "\n",
    "cnn.add_layer(nn.BatchNorm2d(conv_7_out))\n",
    "cnn.add_layer(nn.Dropout2d(conv_8_drop))\n",
    "cnn.add_layer(nn.Conv2d(conv_7_out, conv_8_out, conv_8_kernel, padding=conv_8_pad, stride=conv_8_stride))\n",
    "side_len = int((side_len - conv_8_kernel + (2 * conv_8_pad)) / conv_8_stride) + 1\n",
    "\n",
    "cnn.add_layer(nn.ReLU())\n",
    "# cnn.add_layer(nn.MaxPool2d(2, 2))\n",
    "# side_len = int(side_len / 2)\n",
    "\n",
    "\n",
    "cnn.add_layer(nn.BatchNorm2d(conv_8_out))\n",
    "cnn.add_layer(nn.Dropout2d(conv_9_drop))\n",
    "cnn.add_layer(nn.Conv2d(conv_8_out, conv_9_out, conv_9_kernel, padding=conv_9_pad, stride=conv_9_stride))\n",
    "side_len = int((side_len - conv_9_kernel + (2 * conv_9_pad)) / conv_9_stride) + 1\n",
    "\n",
    "cnn.add_layer(nn.ReLU())\n",
    "# cnn.add_layer(nn.MaxPool2d(2, 2))\n",
    "# side_len = int(side_len / 2)\n",
    "\n",
    "\n",
    "# Bring the data into a line.\n",
    "cnn.add_layer(View_Layer(-1, conv_9_out * side_len**2))\n",
    "\n",
    "# Run a fully connected neural network. Use Dropout layers to set some activations to zero during training.\n",
    "cnn.add_layer(nn.BatchNorm1d(conv_9_out * side_len**2))\n",
    "cnn.add_layer(nn.Dropout(drop_1_p))\n",
    "cnn.add_layer(nn.Linear(conv_9_out * side_len**2, 120))\n",
    "cnn.add_layer(nn.ReLU())\n",
    "cnn.add_layer(nn.BatchNorm1d(120))\n",
    "cnn.add_layer(nn.Dropout(drop_2_p))\n",
    "cnn.add_layer(nn.Linear(120, 84))\n",
    "cnn.add_layer(nn.ReLU())\n",
    "cnn.add_layer(nn.BatchNorm1d(84))\n",
    "cnn.add_layer(nn.Dropout(drop_3_p))\n",
    "cnn.add_layer(nn.Linear(84, 10))\n",
    "\n",
    "# Convert to a Double network, prepare to train, and move to the GPU.\n",
    "cnn.double()\n",
    "cnn.train(True)\n",
    "cnn.cuda()\n",
    "\n",
    "# Fit on the raining data.\n",
    "fit_loader(cnn, loader, 25, lr=0.2)\n",
    "cnn.train(False)\n",
    "acc = get_accuracy(cnn, data[\"x_valid\"], data[\"y_valid\"])\n",
    "\n",
    "# Log the configuration, and the resulting accuracy.\n",
    "log(str(conv_1_out) + \"\\t\" + str(conv_2_out) + \"\\t\" + str(conv_3_out) + \"\\t\" + str(conv_4_out) + \"\\t\" + str(conv_5_out) + \"\\t\" + str(conv_6_out) + \"\\t\" + str(conv_7_out) + \"\\t\" + str(conv_8_out) + \"\\t\" + str(conv_9_out) + \"\\t\" + str(conv_1_kernel))\n",
    "log(\"Accuracy: \" + str(acc))\n",
    "log(\"Train Accuracy: \" + str(get_accuracy(cnn, data[\"x_train\"], data[\"y_train\"])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the CNN on the full training set, and make predictions for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# loading test data\n",
    "x_train_full = clean_data(np.loadtxt(\"data/train_x.csv\", delimiter=\",\"))\n",
    "y_train_full = np.loadtxt(\"data/train_y.csv\", delimiter=\",\")\n",
    "y_train_one_hot = to_one_hot(y_train)\n",
    "x_test_full = clean_data(np.loadtxt(\"data/test_x.csv\", delimiter=\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(filename, y_pred):\n",
    "    \"\"\"\n",
    "    Save the predictions to a file.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"Id,Label\")\n",
    "        for i in range(0, len(y_pred)):\n",
    "            f.write(\"\\n\" + str(i) + \",\" + str(y_pred[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log(\"Testing with 9 conv layers.\")\n",
    "\n",
    "x_tr = x_train_full.reshape(-1, 1, 64, 64)\n",
    "y_tr = y_train_full\n",
    "\n",
    "# Add a random rotation to the data.\n",
    "trans = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: np.array([255 if i == 1.0 else 0 for i in x.reshape(-1)]).astype(np.uint8).reshape(64, 64, 1)),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomRotation(40),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.view(1, 64, 64).double().apply_(lambda a: 1.0 if a == 1 else -1.0))\n",
    "])\n",
    "\n",
    "train = ImageArrayDataset(x_tr, y_tr, transform=trans)\n",
    "\n",
    "loader = torch.utils.data.DataLoader(train, batch_size=500, shuffle=True, num_workers=2)\n",
    "\n",
    "# Set the padding for each layer.\n",
    "conv_1_pad = 2\n",
    "conv_2_pad = 2\n",
    "conv_3_pad = 2\n",
    "conv_4_pad = 1\n",
    "conv_5_pad = 1\n",
    "conv_6_pad = 0\n",
    "conv_7_pad = 0\n",
    "conv_8_pad = 0\n",
    "conv_9_pad = 0\n",
    "\n",
    "# Set the stride for each layer.\n",
    "conv_1_stride = 1\n",
    "conv_2_stride = 1\n",
    "conv_3_stride = 1\n",
    "conv_4_stride = 1\n",
    "conv_5_stride = 1\n",
    "conv_6_stride = 1\n",
    "conv_7_stride = 1\n",
    "conv_8_stride = 1\n",
    "conv_9_stride = 1\n",
    "\n",
    "# Set the filter size for each layer.\n",
    "conv_1_kernel = 3\n",
    "conv_2_kernel = 3\n",
    "conv_3_kernel = 3\n",
    "conv_4_kernel = 3\n",
    "conv_5_kernel = 3\n",
    "conv_6_kernel = 3\n",
    "conv_7_kernel = 3\n",
    "conv_8_kernel = 3\n",
    "conv_9_kernel = 3\n",
    "\n",
    "# Set the probability of data being dropped before each convolution layer.\n",
    "conv_2_drop = 0.1\n",
    "conv_3_drop = 0.1\n",
    "conv_4_drop = 0.1\n",
    "conv_5_drop = 0.1\n",
    "conv_6_drop = 0.1\n",
    "conv_7_drop = 0.1\n",
    "conv_8_drop = 0.1\n",
    "conv_9_drop = 0.1\n",
    "\n",
    "# Set the number of output channels for each convolution layer.\n",
    "conv_1_out = 25\n",
    "conv_2_out = 35\n",
    "conv_3_out = 50\n",
    "conv_4_out = 65\n",
    "conv_5_out = 80\n",
    "conv_6_out = 90\n",
    "conv_7_out = 100\n",
    "conv_8_out = 110\n",
    "conv_9_out = 120\n",
    "\n",
    "# Set the probability of dropping data before each linear layer.\n",
    "drop_1_p = 0.5\n",
    "drop_2_p = 0.5\n",
    "drop_3_p = 0.5\n",
    "\n",
    "\n",
    "# Initialize CNN.\n",
    "cnn = CNN()\n",
    "\n",
    "# Add batch normalization, and then add the convolution layer.\n",
    "cnn.add_layer(nn.BatchNorm2d(1))\n",
    "cnn.add_layer(nn.Conv2d(1, conv_1_out, conv_1_kernel, padding=conv_1_pad, stride=conv_1_stride))\n",
    "\n",
    "# Track the adjustment to the side length of the output image.\n",
    "side_len = int((64 - conv_1_kernel + (2 * conv_1_pad)) / conv_1_stride) + 1\n",
    "\n",
    "# Use ReLU for activation, and then max pool.\n",
    "cnn.add_layer(nn.ReLU())\n",
    "cnn.add_layer(nn.MaxPool2d(2,2, padding=1))\n",
    "side_len = int((side_len + 2) / 2)\n",
    "\n",
    "\n",
    "# Repeat for each convolution layer.\n",
    "\n",
    "cnn.add_layer(nn.BatchNorm2d(conv_1_out))\n",
    "cnn.add_layer(nn.Dropout2d(conv_2_drop))\n",
    "cnn.add_layer(nn.Conv2d(conv_1_out, conv_2_out, conv_2_kernel, padding=conv_2_pad, stride=conv_2_stride))\n",
    "side_len = int((side_len - conv_2_kernel + (2 * conv_2_pad)) / conv_2_stride) + 1\n",
    "\n",
    "cnn.add_layer(nn.ReLU())\n",
    "cnn.add_layer(nn.MaxPool2d(2, 2, padding=1))\n",
    "side_len = int((side_len + 2) / 2)\n",
    "\n",
    "\n",
    "cnn.add_layer(nn.BatchNorm2d(conv_2_out))\n",
    "cnn.add_layer(nn.Dropout2d(conv_3_drop))\n",
    "cnn.add_layer(nn.Conv2d(conv_2_out, conv_3_out, conv_3_kernel, padding=conv_3_pad, stride=conv_3_stride))\n",
    "side_len = int((side_len - conv_3_kernel + (2 * conv_3_pad)) / conv_3_stride) + 1\n",
    "\n",
    "cnn.add_layer(nn.ReLU())\n",
    "cnn.add_layer(nn.MaxPool2d(2, 2, padding=1))\n",
    "side_len = int((side_len + 2) / 2)\n",
    "\n",
    "\n",
    "cnn.add_layer(nn.BatchNorm2d(conv_3_out))\n",
    "cnn.add_layer(nn.Dropout2d(conv_4_drop))\n",
    "cnn.add_layer(nn.Conv2d(conv_3_out, conv_4_out, conv_4_kernel, padding=conv_4_pad, stride=conv_4_stride))\n",
    "side_len = int((side_len - conv_4_kernel + (2 * conv_4_pad)) / conv_4_stride) + 1\n",
    "\n",
    "# Leave the option open for more Max Pooling.\n",
    "cnn.add_layer(nn.ReLU())\n",
    "# cnn.add_layer(nn.MaxPool2d(2, 2))\n",
    "# side_len = int(side_len / 2)\n",
    "\n",
    "\n",
    "cnn.add_layer(nn.BatchNorm2d(conv_4_out))\n",
    "cnn.add_layer(nn.Dropout2d(conv_5_drop))\n",
    "cnn.add_layer(nn.Conv2d(conv_4_out, conv_5_out, conv_5_kernel, padding=conv_5_pad, stride=conv_5_stride))\n",
    "side_len = int((side_len - conv_5_kernel + (2 * conv_5_pad)) / conv_5_stride) + 1\n",
    "\n",
    "cnn.add_layer(nn.ReLU())\n",
    "# cnn.add_layer(nn.MaxPool2d(2, 2))\n",
    "# side_len = int(side_len / 2)\n",
    "\n",
    "\n",
    "cnn.add_layer(nn.BatchNorm2d(conv_5_out))\n",
    "cnn.add_layer(nn.Dropout2d(conv_6_drop))\n",
    "cnn.add_layer(nn.Conv2d(conv_5_out, conv_6_out, conv_6_kernel, padding=conv_6_pad, stride=conv_6_stride))\n",
    "side_len = int((side_len - conv_6_kernel + (2 * conv_6_pad)) / conv_6_stride) + 1\n",
    "\n",
    "cnn.add_layer(nn.ReLU())\n",
    "# cnn.add_layer(nn.MaxPool2d(2, 2))\n",
    "# side_len = int(side_len / 2)\n",
    "\n",
    "\n",
    "cnn.add_layer(nn.BatchNorm2d(conv_6_out))\n",
    "cnn.add_layer(nn.Dropout2d(conv_7_drop))\n",
    "cnn.add_layer(nn.Conv2d(conv_6_out, conv_7_out, conv_7_kernel, padding=conv_7_pad, stride=conv_7_stride))\n",
    "side_len = int((side_len - conv_7_kernel + (2 * conv_7_pad)) / conv_7_stride) + 1\n",
    "\n",
    "cnn.add_layer(nn.ReLU())\n",
    "# cnn.add_layer(nn.MaxPool2d(2, 2))\n",
    "# side_len = int(side_len / 2)\n",
    "\n",
    "\n",
    "cnn.add_layer(nn.BatchNorm2d(conv_7_out))\n",
    "cnn.add_layer(nn.Dropout2d(conv_8_drop))\n",
    "cnn.add_layer(nn.Conv2d(conv_7_out, conv_8_out, conv_8_kernel, padding=conv_8_pad, stride=conv_8_stride))\n",
    "side_len = int((side_len - conv_8_kernel + (2 * conv_8_pad)) / conv_8_stride) + 1\n",
    "\n",
    "cnn.add_layer(nn.ReLU())\n",
    "# cnn.add_layer(nn.MaxPool2d(2, 2))\n",
    "# side_len = int(side_len / 2)\n",
    "\n",
    "\n",
    "cnn.add_layer(nn.BatchNorm2d(conv_8_out))\n",
    "cnn.add_layer(nn.Dropout2d(conv_9_drop))\n",
    "cnn.add_layer(nn.Conv2d(conv_8_out, conv_9_out, conv_9_kernel, padding=conv_9_pad, stride=conv_9_stride))\n",
    "side_len = int((side_len - conv_9_kernel + (2 * conv_9_pad)) / conv_9_stride) + 1\n",
    "\n",
    "cnn.add_layer(nn.ReLU())\n",
    "# cnn.add_layer(nn.MaxPool2d(2, 2))\n",
    "# side_len = int(side_len / 2)\n",
    "\n",
    "\n",
    "# Bring the data into a line.\n",
    "cnn.add_layer(View_Layer(-1, conv_9_out * side_len**2))\n",
    "\n",
    "# Run a fully connected neural network. Use Dropout layers to set some activations to zero during training.\n",
    "cnn.add_layer(nn.BatchNorm1d(conv_9_out * side_len**2))\n",
    "cnn.add_layer(nn.Dropout(drop_1_p))\n",
    "cnn.add_layer(nn.Linear(conv_9_out * side_len**2, 120))\n",
    "cnn.add_layer(nn.ReLU())\n",
    "cnn.add_layer(nn.BatchNorm1d(120))\n",
    "cnn.add_layer(nn.Dropout(drop_2_p))\n",
    "cnn.add_layer(nn.Linear(120, 84))\n",
    "cnn.add_layer(nn.ReLU())\n",
    "cnn.add_layer(nn.BatchNorm1d(84))\n",
    "cnn.add_layer(nn.Dropout(drop_3_p))\n",
    "cnn.add_layer(nn.Linear(84, 10))\n",
    "\n",
    "# Convert to a Double network, prepare to train, and move to the GPU.\n",
    "cnn.double()\n",
    "cnn.train(True)\n",
    "cnn.cuda()\n",
    "\n",
    "# Fit on the raining data.\n",
    "fit_loader(cnn, loader, 25, lr=0.2)\n",
    "cnn.train(False)\n",
    "\n",
    "# Log the configuration, and the resulting accuracy.\n",
    "log(str(conv_1_out) + \"\\t\" + str(conv_2_out) + \"\\t\" + str(conv_3_out) + \"\\t\" + str(conv_4_out) + \"\\t\" + str(conv_5_out) + \"\\t\" + str(conv_6_out) + \"\\t\" + str(conv_7_out) + \"\\t\" + str(conv_8_out) + \"\\t\" + str(conv_9_out) + \"\\t\" + str(conv_1_kernel))\n",
    "log(\"Train Accuracy: \" + str(get_accuracy(cnn, data[\"x_train\"], data[\"y_train\"])))\n",
    "save_results(\"test_y_94.csv\", from_one_hot(predict_batch(cnn, x_test_full)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-353)",
   "language": "python",
   "name": "pytorch-353"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
