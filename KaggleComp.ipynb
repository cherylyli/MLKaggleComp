{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import sklearn \n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "x_train = np.loadtxt(\"data/train_x.csv\", delimiter=\",\")\n",
    "y_train = np.loadtxt(\"data/train_y.csv\", delimiter=\",\")\n",
    "x_test = np.loadtxt(\"data/test_x.csv\", delimiter=\",\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode y_train data in one-hot encoding\n",
    "def to_one_hot(y):\n",
    "    y_train_one_hot = [[0 for i in range(10)] for i in range(len(y))]\n",
    "    for i in range(len(y)):\n",
    "        y_train_one_hot[i][int(y[i])] = 1\n",
    "\n",
    "    return np.array(y_train_one_hot)\n",
    "\n",
    "def from_one_hot(one_hot_data):\n",
    "    y = []\n",
    "    for row in one_hot_data:\n",
    "        y.append(np.argmax(row))\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(img):\n",
    "    plt.close()\n",
    "    plt.imshow(img, cmap='gray_r')\n",
    "    plt.show()\n",
    "\n",
    "x_train_reshaped = x_train.reshape(-1, 64, 64)\n",
    "y_train_reshaped = y_train.reshape(-1, 1) \n",
    "x_test_reshaped = x_test.reshape(-1, 64, 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(x):\n",
    "    \"\"\"\n",
    "    Because the only thing that matters is the numbers in the picture, which are black, \n",
    "    we recode the pixels as 1 if the pixel was 255 and 0 otherwise. To reduce noise and to\n",
    "    reduce overflow/underflow in later stages.\n",
    "    \"\"\"\n",
    "    new_data = []\n",
    "    for row in x:\n",
    "        new_row = []\n",
    "        for pixel in row:\n",
    "            if pixel == 255:\n",
    "                new_row.append(1.0)\n",
    "            else:\n",
    "                new_row.append(-1.0)\n",
    "        new_data.append(new_row)\n",
    "    return np.array(new_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split training data into train / valid sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train_s, x_valid_s, y_train_s, y_valid_s = train_test_split(x_train, y_train, train_size=0.8, test_size=0.2)\n",
    "data = {\n",
    "    \"x_train\": clean_data(x_train_s),\n",
    "    \"x_valid\": clean_data(x_valid_s),\n",
    "    \"y_train\": to_one_hot(y_train_s),\n",
    "    \"y_valid\": to_one_hot(y_valid_s),\n",
    "    \"y_train_og\": y_train_s,\n",
    "    \"y_valid_og\": y_valid_s,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWsj/B4A+3MKOrMiKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19211406d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqoX0lNhnTJAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19233afc50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkMGXGmTwpQYZfKlBBl9q0P8CI0NbPwdAd1gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x192116c7f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# x_cleaned_reshaped = data[\"x_train\"].reshape(-1, 64, 64)\n",
    "\n",
    "# x_train_s_reshaped = x_train_s.reshape(-1, 64, 64)\n",
    "# show_img(x_train_s_reshaped[0])\n",
    "# show_img(x_cleaned_reshaped[0])\n",
    "# x_v_cleaned_reshaped = data[\"x_valid\"].reshape(-1, 64, 64)\n",
    "# show_img(x_v_cleaned_reshaped[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Linear Classifier: Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 4096)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_linear_svm(data):\n",
    "    \"\"\"\n",
    "    Using out-of-the-box linear SVM to classify data\n",
    "    \"\"\"\n",
    "    clf = LinearSVC()\n",
    "    \n",
    "    y_pred = clf.fit(data[\"x_train\"], data[\"y_train\"]).predict(data[\"x_valid\"])\n",
    "    print(y_pred)\n",
    "    return metrics.accuracy_score(data[\"y_valid\"], y_pred, average=\"macro\"), y_pred\n",
    "    \n",
    "# score, y_pred = baseline_linear_svm(data)\n",
    "# print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def log_loss(output):\n",
    "#     loss = []\n",
    "#     for i in range(len(output)):\n",
    "#         log_loss_sum = []\n",
    "#         for j in range(10):\n",
    "#             log_loss_sum.append(np.log(output[i][j]))\n",
    "#         loss.append(log_loss_sum)\n",
    "#     return np.array(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_rows, input_cols, learning_rate=0.01, num_nodes=200, activation_func=\"sigmoid\"):\n",
    "        self.input_rows = input_rows\n",
    "        self.input_cols = input_cols\n",
    "        self.num_nodes = num_nodes\n",
    "        self.next = None\n",
    "        self.prev = None\n",
    "        input_range = 1.0 / input_rows ** (1/2)\n",
    "        self.w = np.random.normal(loc=0, scale=input_range, size=(self.input_cols,num_nodes))\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if activation_func == \"sigmoid\":\n",
    "            self.activation_func = expit\n",
    "            self.d_activation_func = lambda x: x * (1. - x)\n",
    "            \n",
    "        elif activation_func == \"tanh\":\n",
    "            self.activation_func = lambda x: np.tanh(x)\n",
    "            self.d_activation_func = lambda x: 1 - np.square(x)\n",
    "            \n",
    "        elif activation_func == \"relu\":\n",
    "            self.activation_func = lambda x: np.maximum(0,x)\n",
    "            self.d_activation_func = lambda x: x/x\n",
    "        else:\n",
    "            raise Error\n",
    "        \n",
    "    def feedforward(self, x):\n",
    "        \"\"\"\n",
    "        return the predictions (represented by a probability)\n",
    "        \"\"\"\n",
    "        # calculate stuff\n",
    "        self.input = x\n",
    "        before_activation = np.dot(x, self.w)\n",
    "        self.output = self.activation_func(before_activation) \n",
    "        self.derivative = self.d_activation_func(before_activation)\n",
    "        \n",
    "        # if there's a next layer\n",
    "        if self.next:\n",
    "            passed_output = []\n",
    "            # add bias to the end of each row of self.output\n",
    "            try:\n",
    "                passed_output = np.append(self.output, np.ones((self.output.shape[0], 1)), axis=-1)\n",
    "            except ValueError:\n",
    "                passed_output = np.append(self.output, 1) \n",
    "            \n",
    "            # call next layer's feedforward step\n",
    "            self.next.feedforward(passed_output)\n",
    " \n",
    "    def backprop(self, prev_deltas):\n",
    "        \"\"\"\n",
    "        compute derivatives and adjust w\n",
    "        \"\"\" \n",
    "        deltas = prev_deltas * self.derivative\n",
    "        if self.prev:\n",
    "            self.prev.backprop(np.dot(self.w[:-1], deltas.T).T)\n",
    "        self.w = self.w - (self.learning_rate * np.dot(self.input.T, deltas))\n",
    "\n",
    "class OutputLayer(Layer):\n",
    "    def __init__(self, input_rows, input_cols, learning_rate=0.01, num_nodes=200, activation_func=\"softmax\"):\n",
    "        self.input_rows = input_rows\n",
    "        self.input_cols = input_cols\n",
    "        self.num_nodes = num_nodes\n",
    "        self.next = None\n",
    "        self.prev = None\n",
    "        input_range = 1.0 / input_rows ** (1/2)\n",
    "        self.w = np.random.normal(loc=0, scale=input_range, size=(self.input_cols,num_nodes))\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if activation_func == \"softmax\":\n",
    "            self.activation_func = lambda x: 1 / (1 + np.exp(-x))\n",
    "            self.backprop_func = lambda x, target: x - target\n",
    "            \n",
    "        elif activation_func == \"sigmoid\":\n",
    "            self.activation_func = self.sigmoid\n",
    "            self.backprop_func = lambda x, target: self.d_sigmoid(x) * (x - target)\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # Value is unused.\n",
    "        self.d_activation_func = lambda x: None\n",
    "     \n",
    "    def backprop(self, targets):\n",
    "        deltas = self.backprop_func(self.output, targets)\n",
    "        self.prev.backprop(np.dot(self.w[:-1], deltas.T).T)\n",
    "        self.w = self.w - self.learning_rate * np.dot(self.input.T, deltas)\n",
    "        \n",
    "    \n",
    "    def softmax(self, x):\n",
    "        y = []\n",
    "        for i in range(len(x)):\n",
    "            row_exp = np.exp(x[i] - np.amax(x[i]))\n",
    "            row_sum = np.sum(row_exp)\n",
    "            y.append(row_exp/row_sum)\n",
    "        return np.array(y)\n",
    "        \n",
    "    \n",
    "class NeuralNet:\n",
    "    def __init__(self, learning_rate, num_epochs):\n",
    "        self.first = None\n",
    "        self.last = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "    def add_layer(self, layer):\n",
    "        \"\"\"\n",
    "        Add layer to the end\n",
    "        \"\"\"\n",
    "        if not self.first:\n",
    "            self.first = layer\n",
    "            self.last = layer\n",
    "            \n",
    "        else:\n",
    "            temp = self.last\n",
    "            temp.next = layer\n",
    "            layer.prev = temp\n",
    "            self.last = layer\n",
    "            \n",
    "    def fit(self, x_train, y_train, x_valid, y_valid):\n",
    "        print(x_valid.shape, y_valid.shape)\n",
    "        x_input = np.append(x_train, np.ones((x_train.shape[0], 1)), axis=-1)\n",
    "        init_acc = self.get_accuracy(x_valid, y_valid)\n",
    "        \n",
    "        for i in range(self.num_epochs):\n",
    "\n",
    "            for j in range(0, len(x_input), 1000):\n",
    "                self.first.feedforward(x_input[j:j+1000])\n",
    "                self.last.backprop(y_train[j:j+1000])\n",
    "                \n",
    "#             if (i+1)%25 == 0:\n",
    "#                 print(i+1, \"after feedforward & backprop\")\n",
    "#                 print(self.get_accuracy(x_valid))\n",
    "                \n",
    "        final_acc = self.get_accuracy(x_valid, y_valid)\n",
    "        print(\"Initial accuracy, final accuracy\", init_acc, final_acc)\n",
    "        return final_acc\n",
    "                \n",
    "                \n",
    "    def predict(self, x):\n",
    "        x_input = np.append(x, np.ones((x.shape[0], 1)), axis=-1)\n",
    "        self.first.feedforward(x_input)\n",
    "        return self.last.output\n",
    "    \n",
    "    def get_accuracy(self, x_valid, y_valid):\n",
    "        y_pred = self.predict(x_valid)\n",
    "        return metrics.accuracy_score(from_one_hot(y_valid), from_one_hot(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1317\n"
     ]
    }
   ],
   "source": [
    "x_tr = data[\"x_train\"]\n",
    "y_tr = data[\"y_train\"]\n",
    "\n",
    "\n",
    "# neural_net = NeuralNet(1e-5, 2)\n",
    "# # neural_net.k_fold(x_train, y_train)\n",
    "# neural_net.add_layer(Layer(x_tr.shape[0], x_tr.shape[1] + 1, 1e-5, 3500, activation_func=\"relu\"))\n",
    "# # neural_net.add_layer(Layer(x_tr.shape[0], 300 + 1, 1e-6, 200))\n",
    "# neural_net.add_layer(OutputLayer(x_tr.shape[0], 3500 + 1, 1e-5, 10))\n",
    "# # neural_net.fit(x_tr, y_tr, data[\"x_valid\"], data[\"y_valid\"])\n",
    "# # print(\"Done\")\n",
    "        \n",
    "def k_fold(x, y):\n",
    "    # divide into 5 folds\n",
    "    y = to_one_hot(y)\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    kf.get_n_splits(x)\n",
    "    accuracy_scores = []\n",
    "\n",
    "    # for each fold, train and compute accuracy\n",
    "    for train_index, test_index in kf.split(x):\n",
    "\n",
    "        # create 'new' train & valid sets based on kfold split\n",
    "        new_xt, new_yt = [], []\n",
    "        new_xv, new_yv = [], []\n",
    "        for ind in train_index:\n",
    "            new_xt.append(x[ind])\n",
    "            new_yt.append(y[ind])\n",
    "\n",
    "        for ind in test_index:\n",
    "            new_xv.append(x[ind])\n",
    "            new_yv.append(y[ind])\n",
    "        \n",
    "        \n",
    "        new_xt = np.array(new_xt)\n",
    "        new_yt = np.array(new_yt)\n",
    "        new_xv = np.array(new_xv)\n",
    "        new_yv = np.array(new_yv)\n",
    "        print(new_xt.shape, new_yt.shape, new_xv.shape, new_yv.shape)\n",
    "        \n",
    "        nn = NeuralNet(1e-5, 500)\n",
    "        nn.add_layer(Layer(new_xt.shape[0], new_xt.shape[1] + 1, 1e-5, 3000, activation_func=\"relu\"))\n",
    "        nn.add_layer(OutputLayer(new_xt.shape[0], 3000 + 1, 1e-5, 10))\n",
    "        acc_score = nn.fit(new_xt, new_yt, new_xv, new_yv)\n",
    "        accuracy_scores.append(acc_score)\n",
    "\n",
    "    print(accuracy_scores, np.average(accuracy_scores))\n",
    "    return accuracy_scores\n",
    "\n",
    "k_fold(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Neural Net with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-377057c9f24f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0mneural_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTorchOutputLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m75\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m \u001b[0mneural_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;31m# predict_y = neural_net.predict(data[\"x_valid\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-377057c9f24f>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x_train, y_train)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;31m#             print(self.last.w)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeedforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-353/lib/python3.5/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class TorchLayer:\n",
    "    def __init__(self, input_rows, input_cols, learning_rate=0.01, num_nodes=200, activation_func=\"sigmoid\"):\n",
    "        self.input_rows = input_rows\n",
    "        self.input_cols = input_cols\n",
    "        self.num_nodes = num_nodes\n",
    "        self.next = None\n",
    "        self.prev = None\n",
    "        input_range = 1.0 / input_rows ** (1/2)\n",
    "        self.w = torch.from_numpy(np.random.normal(loc=0, scale=input_range, size=(self.input_cols,num_nodes))).cuda()\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if activation_func == \"sigmoid\":\n",
    "            self.activation_func = torch.sigmoid\n",
    "            self.d_activation_func = self.d_sigmoid\n",
    "            \n",
    "            \n",
    "        elif activation_func == \"tanh\":\n",
    "            self.activation_func = self.tanh\n",
    "            self.d_activation_func = self.d_tanh\n",
    "            \n",
    "        elif activation_func == \"relu\":\n",
    "            self.activation_func = self.relu\n",
    "            self.d_activation_func = self.d_relu\n",
    "        else:\n",
    "            raise Error\n",
    "        \n",
    "    def feedforward(self, x):\n",
    "        \"\"\"\n",
    "        return the predictions (represented by a probability)\n",
    "        \"\"\"\n",
    "        # calculate stuff\n",
    "        self.input = x\n",
    "        before_activation = torch.mm(x, self.w)\n",
    "        self.output = self.activation_func(before_activation) \n",
    "        self.derivative = self.d_activation_func(before_activation)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # if there's a next layer\n",
    "        if self.next: \n",
    "            passed_output = torch.cat((self.output, torch.ones(self.output.size()[0], 1).double().cuda()), 1)\n",
    "            # add bias to the end of each row of self.output\n",
    "            # try:\n",
    "            #     passed_output = np.append(self.output, np.ones((self.output.shape[0], 1)), axis=-1)\n",
    "            # except ValueError:\n",
    "            #     passed_output = np.append(self.output, 1) \n",
    "            \n",
    "            # call next layer's feedforward step\n",
    "            self.next.feedforward(passed_output)\n",
    "\n",
    "        \n",
    "    def backprop(self, prev_deltas):\n",
    "        \"\"\"\n",
    "        compute derivatives and adjust w\n",
    "        \"\"\" \n",
    "#         deltas = np.dot(self.derivative, prev_deltas )\n",
    "        deltas = prev_deltas * self.derivative\n",
    "        if self.prev:\n",
    "            self.prev.backprop(torch.mm(self.w[:-1], deltas.t()).t())\n",
    "        self.w = self.w - (self.learning_rate * torch.mm(self.input.t(), deltas))\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return torch.tanh(x)\n",
    "        \n",
    "    def d_tanh(self, x):\n",
    "        return 1 - (torch.tanh(x))**2\n",
    "        \n",
    "    def relu(self, x):\n",
    "        return torch.clamp(x, min=0)\n",
    "    \n",
    "    def d_relu(self, x):\n",
    "        return torch.gt(x, 0).double()\n",
    "    \n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        sigmoid function\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def d_sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        derivative of sigmoid\n",
    "        \"\"\"\n",
    "        return x * (1. - x)\n",
    "   \n",
    "    \n",
    "class TorchOutputLayer(TorchLayer):\n",
    "    def __init__(self, input_rows, input_cols, learning_rate=0.01, num_nodes=200, activation_func=\"softmax\"):\n",
    "        self.input_rows = input_rows\n",
    "        self.input_cols = input_cols\n",
    "        self.num_nodes = num_nodes\n",
    "        self.next = None\n",
    "        self.prev = None\n",
    "        self.w = torch.from_numpy(np.random.uniform(size=(self.input_cols,num_nodes)) / np.sqrt(self.input_cols)).cuda()\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if activation_func == \"softmax\":\n",
    "            self.activation_func = self.softmax\n",
    "            self.backprop_func = lambda x, target: x - target\n",
    "            \n",
    "        elif activation_func == \"sigmoid\":\n",
    "            self.activation_func = self.sigmoid\n",
    "            self.backprop_func = lambda x, target: self.d_sigmoid(x) * (x - target)\n",
    "            \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        # Value is unused.\n",
    "        self.d_activation_func = lambda x: None\n",
    "     \n",
    "    def backprop(self, targets):\n",
    "        deltas = self.backprop_func(self.output, targets)\n",
    "        self.prev.backprop(torch.mm(self.w[:-1], deltas.t()).t())\n",
    "        self.w = self.w - self.learning_rate * torch.mm(self.input.t(), deltas)\n",
    "        \n",
    "    \n",
    "    def softmax(self, x):\n",
    "#         e = np.exp(x - np.amax(x))\n",
    "#         dist = e / np.sum(e)\n",
    "#         return dist\n",
    "#         e = np.exp(x-np.amax(x))\n",
    "#         e_sum = np.sum(e, axis=1)\n",
    "#         y = []\n",
    "#         for i in range(len(x)):\n",
    "#             row_exp = np.exp(x[i] - np.amax(x[i]))\n",
    "#             row_sum = np.sum(row_exp)\n",
    "#             y.append(row_exp/row_sum)\n",
    "#         return np.array(y)\n",
    "        return torch.nn.functional.softmax(torch.autograd.Variable(x), 1).data\n",
    "        \n",
    "    \n",
    "class TorchNeuralNet:\n",
    "    def __init__(self, learning_rate, num_epochs):\n",
    "        self.first = None\n",
    "        self.last = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "    def add_layer(self, layer):\n",
    "        \"\"\"\n",
    "        Add layer to the end\n",
    "        \"\"\"\n",
    "        if not self.first:\n",
    "            self.first = layer\n",
    "            self.last = layer\n",
    "            \n",
    "        else:\n",
    "            temp = self.last\n",
    "            temp.next = layer\n",
    "            layer.prev = temp\n",
    "            self.last = layer\n",
    "            \n",
    "    def fit(self, x_train, y_train):\n",
    "        x_input = np.append(x_train, np.ones((x_train.shape[0], 1)), axis=-1)\n",
    "        # print(\"Initial accuracy\")\n",
    "        # print(self.get_accuracy())\n",
    "        \n",
    "        for i in range(self.num_epochs):\n",
    "\n",
    "#             print(i, \"before feedforward & backprop\")\n",
    "#             print(self.first.w)\n",
    "#             print(self.first.next.w)\n",
    "#             print(self.last.w)\n",
    "            for j in range(0, len(x_input), 50):\n",
    "                self.first.feedforward(torch.from_numpy(x_input[j:j+500]).cuda())\n",
    "                self.last.backprop(torch.from_numpy(y_train[j:j+50]).double().cuda())\n",
    "            \n",
    "            #     \n",
    "            # print(i, \"after feedforward & backprop\")\n",
    "            # print(self.first.w)\n",
    "            # print(self.first.next.w)\n",
    "            # print(self.last.w)\n",
    "            if (i+1)%25 == 0:\n",
    "                print(i+1, \"after feedforward & backprop\")\n",
    "                print(self.get_accuracy())\n",
    "#                 print(self.get_train_accuracy())\n",
    "                \n",
    "        print(self.get_accuracy())\n",
    "        print(self.get_train_accuracy())\n",
    "                \n",
    "    def predict(self, x):\n",
    "        x_input = torch.from_numpy(np.append(x, np.ones((x.shape[0], 1)), axis=-1)).cuda()\n",
    "        self.first.feedforward(x_input)\n",
    "        return self.last.output.cpu().numpy()\n",
    "    \n",
    "    def get_accuracy(self):\n",
    "        y_pred = np.empty((len(data[\"x_valid\"]), 10), dtype=data[\"x_valid\"].dtype)\n",
    "        for i in range(0, len(data[\"x_valid\"]), 1000):\n",
    "            y_pred[i:i+1000] = self.predict(data[\"x_valid\"][i:i+1000])\n",
    "            \n",
    "        return metrics.accuracy_score(data[\"y_valid_og\"], from_one_hot(y_pred))\n",
    "    \n",
    "    def get_train_accuracy(self):\n",
    "        y_pred = np.empty((len(data[\"x_train\"]), 10), dtype=data[\"x_train\"].dtype)\n",
    "        for i in range(0, len(data[\"x_train\"]), 1000):\n",
    "            y_pred[i:i+1000] = self.predict(data[\"x_train\"][i:i+1000])\n",
    "            \n",
    "        return metrics.accuracy_score(data[\"y_train_og\"], from_one_hot(y_pred))\n",
    "\n",
    "\n",
    "x_tr = data[\"x_train\"]\n",
    "y_tr = data[\"y_train\"]\n",
    "\n",
    "neural_net = TorchNeuralNet(1e-5, 400)\n",
    "\n",
    "neural_net.add_layer(TorchLayer(x_tr.shape[0], x_tr.shape[1] + 1, 1e-4, 25, activation_func=\"tanh\"))\n",
    "neural_net.add_layer(TorchLayer(x_tr.shape[0], 25 + 1, 1e-4, 75, activation_func=\"relu\"))\n",
    "# neural_net.add_layer(TorchLayer(x_tr.shape[0], 4500 + 1, 1e-3, 3500, activation_func=\"tanh\"))\n",
    "neural_net.add_layer(TorchOutputLayer(x_tr.shape[0], 75 + 1, 1e-4, 10))\n",
    "\n",
    "neural_net.fit(x_tr, y_tr)\n",
    "# predict_y = neural_net.predict(data[\"x_valid\"])\n",
    "print(\"Done\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 after feedforward & backprop\n",
      "0.2732\n",
      "50 after feedforward & backprop\n",
      "0.2939\n",
      "75 after feedforward & backprop\n",
      "0.2941\n",
      "100 after feedforward & backprop\n",
      "0.3078\n",
      "125 after feedforward & backprop\n",
      "0.3084\n",
      "150 after feedforward & backprop\n",
      "0.2909\n",
      "175 after feedforward & backprop\n",
      "0.2872\n",
      "200 after feedforward & backprop\n",
      "0.3222\n",
      "225 after feedforward & backprop\n",
      "0.3145\n",
      "250 after feedforward & backprop\n",
      "0.3108\n",
      "275 after feedforward & backprop\n",
      "0.3208\n",
      "300 after feedforward & backprop\n",
      "0.3007\n",
      "0.3007\n",
      "0.8265\n"
     ]
    }
   ],
   "source": [
    "neural_net.num_epochs = 300\n",
    "neural_net.fit(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del neural_net\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Convolution Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layers = []\n",
    "        self.num_layers = 0\n",
    "    \n",
    "    def add_layer(self, layer):\n",
    "        self.num_layers += 1\n",
    "        self.add_module(str(self.num_layers), layer)\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class View_Layer(nn.Module):\n",
    "    def __init__(self, param1, param2):\n",
    "        super(View_Layer, self).__init__()\n",
    "        self.param1 = param1\n",
    "        self.param2 = param2\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.view(self.param1, self.param2)\n",
    "    \n",
    "    \n",
    "def fit(cnn, x_train, y_train, epochs, lr=0.01, momentum=0.9):\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(cnn.parameters(), lr=lr, momentum=momentum)\n",
    "    \n",
    "    \n",
    "    for i in range(0, epochs):\n",
    "        for j in range(0, x_train.shape[0], 500):\n",
    "            x_in = Variable(torch.from_numpy(x_train[j:j+500]).cuda()).view(-1, 1, 64, 64)\n",
    "            y_in = Variable(torch.from_numpy(y_train[j:j+500]).long().cuda())\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            y_pred = cnn(x_in)\n",
    "            loss = crit(y_pred, y_in)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "#         if i%25 == 0:\n",
    "#             print(i)\n",
    "#             print(get_accuracy(cnn, data[\"x_valid\"], data[\"y_valid\"]))\n",
    "            \n",
    "\n",
    "def predict(cnn, x):\n",
    "    return cnn(Variable(torch.from_numpy(x).cuda().view(-1, 1, 64, 64))).cpu().data.numpy()\n",
    "\n",
    "def get_accuracy(cnn, x_valid, y_valid):\n",
    "    y_pred = np.empty((len(x_valid), 10), dtype=y_valid.dtype)\n",
    "    for i in range(0, len(x_valid), 1000):\n",
    "        y_pred[i:i+1000] = predict(cnn, x_valid[i:i+1000])\n",
    "            \n",
    "    return metrics.accuracy_score(from_one_hot(y_valid), from_one_hot(y_pred))\n",
    "    \n",
    "\n",
    "# cnn = CNN()\n",
    "\n",
    "# cnn.add_layer(nn.Conv2d(1, 12, 5))\n",
    "# cnn.add_layer(nn.ReLU())\n",
    "# cnn.add_layer(nn.MaxPool2d(2,2))\n",
    "# cnn.add_layer(nn.Conv2d(12, 24, 5))\n",
    "# cnn.add_layer(nn.ReLU())\n",
    "# cnn.add_layer(nn.MaxPool2d(2, 2))\n",
    "# cnn.add_layer(View_Layer(-1, 2704))\n",
    "# cnn.add_layer(nn.Linear(2704, 120))\n",
    "# cnn.add_layer(nn.ReLU())\n",
    "# cnn.add_layer(nn.Linear(120, 84))\n",
    "# cnn.add_layer(nn.ReLU())\n",
    "# cnn.add_layer(nn.Linear(84, 10))\n",
    "\n",
    "# cnn.double()\n",
    "# cnn.cuda()\n",
    "\n",
    "# x_tr = data[\"x_train\"]\n",
    "# y_tr = data[\"y_train_og\"]\n",
    "\n",
    "# fit(cnn, x_tr, y_tr, 60)\n",
    "# print(get_accuracy(cnn, data[\"x_valid\"], data[\"y_valid\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "def setup_file_logger(log_file):\n",
    "    hdlr = logging.FileHandler(log_file)\n",
    "    formatter = logging.Formatter('%(asctime)s %(levelname)s %(message)s')\n",
    "    hdlr.setFormatter(formatter)\n",
    "    logger.addHandler(hdlr) \n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "def log(message):\n",
    "    #outputs to Jupyter console\n",
    "    print('{} {}'.format(datetime.datetime.now(), message))\n",
    "    #outputs to file\n",
    "    logger.info(message)\n",
    "\n",
    "setup_file_logger('out.log')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-19 19:26:18.988466 10\t20\t30\t3\n",
      "2018-03-19 19:26:18.989096 Accuracy: 0.6164\n",
      "2018-03-19 19:29:37.583245 10\t20\t30\t4\n",
      "2018-03-19 19:29:37.584097 Accuracy: 0.771\n",
      "2018-03-19 19:32:18.016987 10\t20\t30\t5\n",
      "2018-03-19 19:32:18.017770 Accuracy: 0.7795\n",
      "2018-03-19 19:36:24.258667 15\t25\t32\t3\n",
      "2018-03-19 19:36:24.259274 Accuracy: 0.6234\n",
      "2018-03-19 19:40:09.843407 15\t25\t32\t4\n",
      "2018-03-19 19:40:09.844238 Accuracy: 0.7893\n",
      "2018-03-19 19:43:49.775582 15\t25\t32\t5\n",
      "2018-03-19 19:43:49.776196 Accuracy: 0.7839\n",
      "2018-03-19 19:43:49.776518 Accuracy:0.7893\n",
      "2018-03-19 19:43:49.776791 15\n",
      "2018-03-19 19:43:49.777053 25\n",
      "2018-03-19 19:43:49.777311 0\n",
      "2018-03-19 19:43:49.777585 0\n",
      "2018-03-19 19:43:49.777839 1\n",
      "2018-03-19 19:43:49.778090 1\n",
      "2018-03-19 19:43:49.778338 0\n",
      "2018-03-19 19:43:49.778592 0\n"
     ]
    }
   ],
   "source": [
    "log(\"Testing with 3 conv layers.\")\n",
    "\n",
    "x_tr = data[\"x_train\"]\n",
    "y_tr = data[\"y_train_og\"]\n",
    "\n",
    "\n",
    "best_acc = 0\n",
    "best_1_out = 0\n",
    "best_2_out = 0\n",
    "best_1_kernel = 0\n",
    "best_2_kernel = 0\n",
    "best_1_stride = 0\n",
    "best_2_stride = 0\n",
    "best_1_pad = 0\n",
    "best_2_pad = 0\n",
    "\n",
    "# for conv_2_kernel in [6, 5]:\n",
    "for conv_1_out, conv_2_out, conv_3_out in [(10, 20, 30), (15, 25, 32)]:\n",
    "#     for conv_2_out in range(20, 29, 4):\n",
    "    for conv_1_kernel in range(3, 6):\n",
    "        conv_2_kernel = conv_1_kernel\n",
    "        conv_3_kernel = conv_1_kernel\n",
    "#         for conv_2_kernel in range(6, 7):\n",
    "        for conv_1_stride in range(1, 2):\n",
    "            for conv_2_stride in range(1, 2):\n",
    "                for conv_3_stride in range(1, 2):\n",
    "                    for conv_1_pad in range(0, 1):\n",
    "                        for conv_2_pad in range(0, 1):\n",
    "                            for conv_3_pad in range(0, 1):\n",
    "                                cnn = CNN()\n",
    "\n",
    "\n",
    "                                cnn.add_layer(nn.Conv2d(1, conv_1_out, conv_1_kernel, padding=conv_1_pad, stride=conv_1_stride))\n",
    "                                side_len = int((64 - conv_1_kernel + (2 * conv_1_pad)) / conv_1_stride) + 1\n",
    "\n",
    "                                cnn.add_layer(nn.ReLU())\n",
    "                                cnn.add_layer(nn.MaxPool2d(2,2))\n",
    "                                side_len = int(side_len / 2)\n",
    "\n",
    "                                cnn.add_layer(nn.Conv2d(conv_1_out, conv_2_out, conv_2_kernel, padding=conv_2_pad, stride=conv_2_stride))\n",
    "                                side_len = int((side_len - conv_2_kernel + (2 * conv_2_pad)) / conv_2_stride) + 1\n",
    "\n",
    "                                cnn.add_layer(nn.ReLU())\n",
    "                                cnn.add_layer(nn.MaxPool2d(2, 2))\n",
    "                                side_len = int(side_len / 2)\n",
    "\n",
    "                                cnn.add_layer(nn.Conv2d(conv_2_out, conv_3_out, conv_3_kernel, padding=conv_3_pad, stride=conv_3_stride))\n",
    "                                side_len = int((side_len - conv_3_kernel + (2 * conv_3_pad)) / conv_3_stride) + 1\n",
    "\n",
    "                                cnn.add_layer(nn.ReLU())\n",
    "                                cnn.add_layer(nn.MaxPool2d(2, 2))\n",
    "                                side_len = int(side_len / 2)\n",
    "\n",
    "                                cnn.add_layer(View_Layer(-1, conv_3_out * side_len**2))\n",
    "                                cnn.add_layer(nn.Linear(conv_3_out * side_len**2, 120))\n",
    "                                cnn.add_layer(nn.ReLU())\n",
    "                                cnn.add_layer(nn.Linear(120, 84))\n",
    "                                cnn.add_layer(nn.ReLU())\n",
    "                                cnn.add_layer(nn.Linear(84, 10))\n",
    "\n",
    "                                cnn.double()\n",
    "                                cnn.cuda()\n",
    "\n",
    "                                fit(cnn, x_tr, y_tr, 20)\n",
    "                                acc = get_accuracy(cnn, data[\"x_valid\"], data[\"y_valid\"])\n",
    "                                log(str(conv_1_out) + \"\\t\" + str(conv_2_out) + \"\\t\" + str(conv_3_out) + \"\\t\" + str(conv_1_kernel))\n",
    "                                log(\"Accuracy: \" + str(acc))\n",
    "\n",
    "                                if (acc > best_acc):\n",
    "                                    best_acc = acc\n",
    "                                    best_1_out = conv_1_out\n",
    "                                    best_2_out = conv_2_out\n",
    "                                    best_1_groups = conv_1_kernel\n",
    "                                    best_2_groups = conv_2_kernel\n",
    "                                    best_1_stride = conv_1_stride\n",
    "                                    best_2_stride = conv_2_stride\n",
    "                                    best_1_pad = conv_1_pad\n",
    "                                    best_2_pad = conv_2_pad\n",
    "\n",
    "                                    \n",
    "log(\"Accuracy:\" + str(best_acc))\n",
    "log(str(best_1_out))\n",
    "log(str(best_2_out))\n",
    "log(str(best_1_kernel))\n",
    "log(str(best_2_kernel))\n",
    "log(str(best_1_stride))\n",
    "log(str(best_2_stride))\n",
    "log(str(best_1_pad))\n",
    "log(str(best_2_pad))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-19 20:10:10.608837 Testing with 4 conv layers.\n",
      "2018-03-19 20:10:10.610419 Padding is at 0\n",
      "2018-03-19 20:10:10.610691 Sride is at 1\n",
      "2018-03-19 20:16:01.468806 15\t30\t45\t60\t5\n",
      "2018-03-19 20:16:01.469472 Accuracy: 0.8535\n",
      "2018-03-19 20:20:44.995234 10\t20\t40\t80\t5\n",
      "2018-03-19 20:20:44.996016 Accuracy: 0.8407\n",
      "2018-03-19 20:27:31.291656 20\t30\t40\t50\t5\n",
      "2018-03-19 20:27:31.292282 Accuracy: 0.8559\n",
      "2018-03-19 20:39:30.290464 20\t40\t80\t160\t5\n",
      "2018-03-19 20:39:30.291077 Accuracy: 0.8631\n",
      "2018-03-19 20:45:13.819439 15\t30\t45\t60\t4\n",
      "2018-03-19 20:45:13.820241 Accuracy: 0.8416\n",
      "2018-03-19 20:50:18.450683 10\t20\t40\t80\t4\n",
      "2018-03-19 20:50:18.451296 Accuracy: 0.8025\n",
      "2018-03-19 20:56:11.003733 20\t30\t40\t50\t4\n",
      "2018-03-19 20:56:11.004373 Accuracy: 0.8327\n",
      "2018-03-19 21:06:09.380641 20\t40\t80\t160\t4\n",
      "2018-03-19 21:06:09.381458 Accuracy: 0.8495\n",
      "2018-03-19 21:12:02.861639 15\t30\t45\t60\t3\n",
      "2018-03-19 21:12:02.862245 Accuracy: 0.7438\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-069ba3db337f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"x_valid\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y_valid\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_1_out\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_2_out\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_3_out\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_4_out\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_1_kernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-d2df07a9aa96>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(cnn, x_train, y_train, epochs, lr, momentum)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mx_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0my_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-353/lib/python3.5/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, async)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "log(\"Testing with 4 conv layers.\")\n",
    "\n",
    "x_tr = data[\"x_train\"]\n",
    "y_tr = data[\"y_train_og\"]\n",
    "\n",
    "\n",
    "best_acc = 0\n",
    "best_1_out = 0\n",
    "best_2_out = 0\n",
    "best_1_kernel = 0\n",
    "best_2_kernel = 0\n",
    "best_1_stride = 0\n",
    "best_2_stride = 0\n",
    "best_1_pad = 0\n",
    "best_2_pad = 0\n",
    "\n",
    "# for conv_2_kernel in [6, 5]:\n",
    "# for conv_1_out, conv_2_out, conv_3_out, conv_4_out in [(15, 30, 45, 60), (10, 20, 40, 80)]:\n",
    "#     for conv_2_out in range(20, 29, 4):\n",
    "\n",
    "#         for conv_2_kernel in range(6, 7):\n",
    "    \n",
    "for conv_1_pad in range(0, 3):\n",
    "    conv_2_pad = conv_1_pad\n",
    "    conv_3_pad = conv_2_pad\n",
    "    conv_4_pad = conv_3_pad\n",
    "\n",
    "    log(\"Padding is at \" + str(conv_1_pad))\n",
    "    \n",
    "    for conv_1_stride in range(1, 4):\n",
    "        conv_2_stride = conv_1_stride\n",
    "        conv_3_stride = conv_2_stride\n",
    "        conv_4_stride = conv_3_stride\n",
    "\n",
    "        log(\"Sride is at \" + str(conv_1_stride))\n",
    "        \n",
    "        for conv_1_kernel in range(5, 2, -1):\n",
    "            conv_2_kernel = conv_1_kernel\n",
    "            conv_3_kernel = conv_2_kernel\n",
    "            conv_4_kernel = conv_3_kernel\n",
    "            \n",
    "            for conv_1_out, conv_2_out, conv_3_out, conv_4_out in [(15, 30, 45, 60), (10, 20, 40, 80), (20, 30, 40, 50), (20, 40, 80, 160)]:\n",
    "\n",
    "                cnn = CNN()\n",
    "\n",
    "\n",
    "                cnn.add_layer(nn.Conv2d(1, conv_1_out, conv_1_kernel, padding=conv_1_pad, stride=conv_1_stride))\n",
    "                side_len = int((64 - conv_1_kernel + (2 * conv_1_pad)) / conv_1_stride) + 1\n",
    "\n",
    "                cnn.add_layer(nn.ReLU())\n",
    "                cnn.add_layer(nn.MaxPool2d(2,2))\n",
    "                side_len = int(side_len / 2)\n",
    "\n",
    "#                 print(side_len)\n",
    "                \n",
    "                cnn.add_layer(nn.Conv2d(conv_1_out, conv_2_out, conv_2_kernel, padding=conv_2_pad, stride=conv_2_stride))\n",
    "                side_len = int((side_len - conv_2_kernel + (2 * conv_2_pad)) / conv_2_stride) + 1\n",
    "\n",
    "                cnn.add_layer(nn.ReLU())\n",
    "                cnn.add_layer(nn.MaxPool2d(2, 2))\n",
    "                side_len = int(side_len / 2)\n",
    "\n",
    "#                 print(side_len)\n",
    "                \n",
    "                cnn.add_layer(nn.Conv2d(conv_2_out, conv_3_out, conv_3_kernel, padding=conv_3_pad, stride=conv_3_stride))\n",
    "                side_len = int((side_len - conv_3_kernel + (2 * conv_3_pad)) / conv_3_stride) + 1\n",
    "\n",
    "                cnn.add_layer(nn.ReLU())\n",
    "#                 cnn.add_layer(nn.MaxPool2d(2, 2))\n",
    "#                 side_len = int(side_len / 2)\n",
    "\n",
    "#                 print(side_len)\n",
    "\n",
    "                cnn.add_layer(nn.Conv2d(conv_3_out, conv_4_out, conv_4_kernel, padding=conv_4_pad, stride=conv_4_stride))\n",
    "                side_len = int((side_len - conv_4_kernel + (2 * conv_4_pad)) / conv_4_stride) + 1\n",
    "\n",
    "                cnn.add_layer(nn.ReLU())\n",
    "#                 cnn.add_layer(nn.MaxPool2d(2, 2))\n",
    "#                 side_len = int(side_len / 2)\n",
    "                \n",
    "#                 print(side_len)\n",
    "                \n",
    "#                 print(str(conv_4_out * side_len**2))\n",
    "                \n",
    "                cnn.add_layer(View_Layer(-1, conv_4_out * side_len**2))\n",
    "                cnn.add_layer(nn.Linear(conv_4_out * side_len**2, 120))\n",
    "                cnn.add_layer(nn.ReLU())\n",
    "                cnn.add_layer(nn.Linear(120, 84))\n",
    "                cnn.add_layer(nn.ReLU())\n",
    "                cnn.add_layer(nn.Linear(84, 10))\n",
    "\n",
    "                cnn.double()\n",
    "                cnn.cuda()\n",
    "\n",
    "                fit(cnn, x_tr, y_tr, 25)\n",
    "                acc = get_accuracy(cnn, data[\"x_valid\"], data[\"y_valid\"])\n",
    "                log(str(conv_1_out) + \"\\t\" + str(conv_2_out) + \"\\t\" + str(conv_3_out) + \"\\t\" + str(conv_4_out) + \"\\t\" + str(conv_1_kernel))\n",
    "                log(\"Accuracy: \" + str(acc))\n",
    "\n",
    "                if (acc > best_acc):\n",
    "                    best_acc = acc\n",
    "                    best_1_out = conv_1_out\n",
    "                    best_2_out = conv_2_out\n",
    "                    best_1_groups = conv_1_kernel\n",
    "                    best_2_groups = conv_2_kernel\n",
    "                    best_1_stride = conv_1_stride\n",
    "                    best_2_stride = conv_2_stride\n",
    "                    best_1_pad = conv_1_pad\n",
    "                    best_2_pad = conv_2_pad\n",
    "\n",
    "                                    \n",
    "log(\"Accuracy:\" + str(best_acc))\n",
    "log(str(best_1_out))\n",
    "log(str(best_2_out))\n",
    "log(str(best_1_kernel))\n",
    "log(str(best_2_kernel))\n",
    "log(str(best_1_stride))\n",
    "log(str(best_2_stride))\n",
    "log(str(best_1_pad))\n",
    "log(str(best_2_pad))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-19 21:18:32.572818 Testing with 5 conv layers.\n",
      "2018-03-19 21:18:32.574104 Padding is at 1\n",
      "2018-03-19 21:18:32.574945 Sride is at 1\n",
      "2018-03-19 21:27:18.444962 15\t30\t45\t60\t5\n",
      "2018-03-19 21:27:18.445629 Accuracy: 0.8518\n",
      "2018-03-19 21:36:05.609262 10\t20\t40\t80\t5\n",
      "2018-03-19 21:36:05.609905 Accuracy: 0.8568\n",
      "2018-03-19 21:45:33.901402 20\t30\t40\t50\t5\n",
      "2018-03-19 21:45:33.902025 Accuracy: 0.8147\n",
      "2018-03-19 21:57:49.454577 20\t40\t40\t60\t5\n",
      "2018-03-19 21:57:49.455197 Accuracy: 0.8539\n",
      "2018-03-19 22:07:34.455947 15\t30\t45\t60\t4\n",
      "2018-03-19 22:07:34.456576 Accuracy: 0.8619\n",
      "2018-03-19 22:19:18.476597 10\t20\t40\t80\t4\n",
      "2018-03-19 22:19:18.477231 Accuracy: 0.7982\n",
      "2018-03-19 22:28:17.053423 20\t30\t40\t50\t4\n",
      "2018-03-19 22:28:17.054074 Accuracy: 0.8139\n",
      "2018-03-19 22:39:51.896069 20\t40\t40\t60\t4\n",
      "2018-03-19 22:39:51.896682 Accuracy: 0.8719\n",
      "2018-03-19 22:49:32.888208 15\t30\t45\t60\t3\n",
      "2018-03-19 22:49:32.888964 Accuracy: 0.7772\n",
      "2018-03-19 23:16:01.135539 10\t20\t40\t80\t3\n",
      "2018-03-19 23:16:01.136308 Accuracy: 0.7485\n",
      "2018-03-19 23:24:18.620051 20\t30\t40\t50\t3\n",
      "2018-03-19 23:24:18.620657 Accuracy: 0.8181\n",
      "2018-03-19 23:34:25.028012 20\t40\t40\t60\t3\n",
      "2018-03-19 23:34:25.028775 Accuracy: 0.7031\n",
      "2018-03-19 23:34:25.029000 Sride is at 2\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (45, 1, 1). Calculated output size: (500, -1, -1). Output size is too small.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-85d315847c3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                 \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m                 \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"x_valid\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"y_valid\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_1_out\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_2_out\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_3_out\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_4_out\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_1_kernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-d2df07a9aa96>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(cnn, x_train, y_train, epochs, lr, momentum)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-353/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-d2df07a9aa96>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-353/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-353/lib/python3.5/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 282\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch-353/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0m_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 torch.backends.cudnn.deterministic, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (45, 1, 1). Calculated output size: (500, -1, -1). Output size is too small."
     ]
    }
   ],
   "source": [
    "log(\"Testing with 5 conv layers.\")\n",
    "\n",
    "x_tr = data[\"x_train\"]\n",
    "y_tr = data[\"y_train_og\"]\n",
    "\n",
    "\n",
    "best_acc = 0\n",
    "best_1_out = 0\n",
    "best_2_out = 0\n",
    "best_1_kernel = 0\n",
    "best_2_kernel = 0\n",
    "best_1_stride = 0\n",
    "best_2_stride = 0\n",
    "best_1_pad = 0\n",
    "best_2_pad = 0\n",
    "\n",
    "# for conv_2_kernel in [6, 5]:\n",
    "# for conv_1_out, conv_2_out, conv_3_out, conv_4_out in [(15, 30, 45, 60), (10, 20, 40, 80)]:\n",
    "#     for conv_2_out in range(20, 29, 4):\n",
    "\n",
    "#         for conv_2_kernel in range(6, 7):\n",
    "    \n",
    "for conv_1_pad in range(1, 3):\n",
    "    conv_2_pad = conv_1_pad\n",
    "    conv_3_pad = conv_2_pad\n",
    "    conv_4_pad = conv_3_pad - 1\n",
    "    conv_5_pad = conv_4_pad\n",
    "\n",
    "    log(\"Padding is at \" + str(conv_1_pad))\n",
    "    \n",
    "    for conv_1_stride in range(1, 4):\n",
    "        conv_2_stride = conv_1_stride\n",
    "        conv_3_stride = conv_2_stride\n",
    "        conv_4_stride = conv_3_stride\n",
    "        conv_5_stride = conv_4_stride\n",
    "\n",
    "        log(\"Sride is at \" + str(conv_1_stride))\n",
    "        \n",
    "        for conv_1_kernel in range(5, 2, -1):\n",
    "            conv_2_kernel = conv_1_kernel\n",
    "            conv_3_kernel = conv_2_kernel\n",
    "            conv_4_kernel = conv_3_kernel\n",
    "            conv_5_kernel = conv_4_kernel\n",
    "            \n",
    "            for conv_1_out, conv_2_out, conv_3_out, conv_4_out, conv_5_out in [(15, 30, 45, 60, 75), (10, 20, 40, 80, 160), (20, 30, 40, 50, 60), (20, 40, 40, 60, 80)]:\n",
    "\n",
    "                cnn = CNN()\n",
    "\n",
    "\n",
    "                cnn.add_layer(nn.Conv2d(1, conv_1_out, conv_1_kernel, padding=conv_1_pad, stride=conv_1_stride))\n",
    "                side_len = int((64 - conv_1_kernel + (2 * conv_1_pad)) / conv_1_stride) + 1\n",
    "\n",
    "                cnn.add_layer(nn.ReLU())\n",
    "                cnn.add_layer(nn.MaxPool2d(2,2))\n",
    "                side_len = int(side_len / 2)\n",
    "\n",
    "#                 print(side_len)\n",
    "                \n",
    "                cnn.add_layer(nn.Conv2d(conv_1_out, conv_2_out, conv_2_kernel, padding=conv_2_pad, stride=conv_2_stride))\n",
    "                side_len = int((side_len - conv_2_kernel + (2 * conv_2_pad)) / conv_2_stride) + 1\n",
    "\n",
    "                cnn.add_layer(nn.ReLU())\n",
    "                cnn.add_layer(nn.MaxPool2d(2, 2))\n",
    "                side_len = int(side_len / 2)\n",
    "\n",
    "#                 print(side_len)\n",
    "                \n",
    "                cnn.add_layer(nn.Conv2d(conv_2_out, conv_3_out, conv_3_kernel, padding=conv_3_pad, stride=conv_3_stride))\n",
    "                side_len = int((side_len - conv_3_kernel + (2 * conv_3_pad)) / conv_3_stride) + 1\n",
    "\n",
    "                cnn.add_layer(nn.ReLU())\n",
    "#                 cnn.add_layer(nn.MaxPool2d(2, 2))\n",
    "#                 side_len = int(side_len / 2)\n",
    "\n",
    "#                 print(side_len)\n",
    "\n",
    "                cnn.add_layer(nn.Conv2d(conv_3_out, conv_4_out, conv_4_kernel, padding=conv_4_pad, stride=conv_4_stride))\n",
    "                side_len = int((side_len - conv_4_kernel + (2 * conv_4_pad)) / conv_4_stride) + 1\n",
    "\n",
    "        \n",
    "                cnn.add_layer(nn.ReLU())\n",
    "#                 cnn.add_layer(nn.MaxPool2d(2, 2))\n",
    "#                 side_len = int(side_len / 2)\n",
    "                \n",
    "#                 print(side_len)\n",
    "\n",
    "                cnn.add_layer(nn.Conv2d(conv_4_out, conv_5_out, conv_5_kernel, padding=conv_5_pad, stride=conv_5_stride))\n",
    "                side_len = int((side_len - conv_5_kernel + (2 * conv_5_pad)) / conv_5_stride) + 1\n",
    "\n",
    "                cnn.add_layer(nn.ReLU())\n",
    "#                 cnn.add_layer(nn.MaxPool2d(2, 2))\n",
    "#                 side_len = int(side_len / 2)\n",
    "\n",
    "#                 print(side_len)\n",
    "\n",
    "#                 print(str(conv_5_out * side_len**2))\n",
    "                \n",
    "                cnn.add_layer(View_Layer(-1, conv_5_out * side_len**2))\n",
    "                cnn.add_layer(nn.Linear(conv_5_out * side_len**2, 120))\n",
    "                cnn.add_layer(nn.ReLU())\n",
    "                cnn.add_layer(nn.Linear(120, 84))\n",
    "                cnn.add_layer(nn.ReLU())\n",
    "                cnn.add_layer(nn.Linear(84, 10))\n",
    "\n",
    "                cnn.double()\n",
    "                cnn.cuda()\n",
    "\n",
    "                fit(cnn, x_tr, y_tr, 25)\n",
    "                acc = get_accuracy(cnn, data[\"x_valid\"], data[\"y_valid\"])\n",
    "                log(str(conv_1_out) + \"\\t\" + str(conv_2_out) + \"\\t\" + str(conv_3_out) + \"\\t\" + str(conv_4_out) + \"\\t\" + str(conv_1_kernel))\n",
    "                log(\"Accuracy: \" + str(acc))\n",
    "\n",
    "                if (acc > best_acc):\n",
    "                    best_acc = acc\n",
    "                    best_1_out = conv_1_out\n",
    "                    best_2_out = conv_2_out\n",
    "                    best_1_groups = conv_1_kernel\n",
    "                    best_2_groups = conv_2_kernel\n",
    "                    best_1_stride = conv_1_stride\n",
    "                    best_2_stride = conv_2_stride\n",
    "                    best_1_pad = conv_1_pad\n",
    "                    best_2_pad = conv_2_pad\n",
    "\n",
    "                                    \n",
    "log(\"Accuracy:\" + str(best_acc))\n",
    "log(str(best_1_out))\n",
    "log(str(best_2_out))\n",
    "log(str(best_1_kernel))\n",
    "log(str(best_2_kernel))\n",
    "log(str(best_1_stride))\n",
    "log(str(best_2_stride))\n",
    "log(str(best_1_pad))\n",
    "log(str(best_2_pad))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-19 23:40:40.788495 Testing with 5 conv layers.\n",
      "2018-03-19 23:40:40.789801 Padding is at 0\n",
      "2018-03-19 23:40:40.790825 Sride is at 1\n",
      "2018-03-19 23:45:09.739024 15\t30\t45\t60\t75\t90\t3\n",
      "2018-03-19 23:45:09.739651 Accuracy: 0.9163\n",
      "2018-03-19 23:45:13.835510 Train Accuracy: 0.9904\n",
      "2018-03-19 23:49:15.360713 20\t30\t40\t50\t60\t70\t3\n",
      "2018-03-19 23:49:15.361493 Accuracy: 0.9134\n",
      "2018-03-19 23:49:19.566576 Train Accuracy: 0.9847\n",
      "2018-03-19 23:53:56.624405 20\t40\t40\t60\t60\t80\t3\n",
      "2018-03-19 23:53:56.625181 Accuracy: 0.9168\n",
      "2018-03-19 23:54:01.758031 Train Accuracy: 0.991\n"
     ]
    }
   ],
   "source": [
    "log(\"Testing with 5 conv layers.\")\n",
    "\n",
    "x_tr = data[\"x_train\"]\n",
    "y_tr = data[\"y_train_og\"]\n",
    "\n",
    "\n",
    "best_acc = 0\n",
    "best_1_out = 0\n",
    "best_2_out = 0\n",
    "best_1_kernel = 0\n",
    "best_2_kernel = 0\n",
    "best_1_stride = 0\n",
    "best_2_stride = 0\n",
    "best_1_pad = 0\n",
    "best_2_pad = 0\n",
    "\n",
    "# for conv_2_kernel in [6, 5]:\n",
    "# for conv_1_out, conv_2_out, conv_3_out, conv_4_out in [(15, 30, 45, 60), (10, 20, 40, 80)]:\n",
    "#     for conv_2_out in range(20, 29, 4):\n",
    "\n",
    "#         for conv_2_kernel in range(6, 7):\n",
    "    \n",
    "for conv_1_pad in range(0, 3):\n",
    "    conv_2_pad = conv_1_pad\n",
    "    conv_3_pad = conv_2_pad\n",
    "    conv_4_pad = conv_3_pad\n",
    "    conv_5_pad = conv_4_pad\n",
    "    conv_6_pad = conv_5_pad\n",
    "\n",
    "    log(\"Padding is at \" + str(conv_1_pad))\n",
    "    \n",
    "    for conv_1_stride in range(1, 4):\n",
    "        conv_2_stride = conv_1_stride\n",
    "        conv_3_stride = conv_2_stride\n",
    "        conv_4_stride = conv_3_stride\n",
    "        conv_5_stride = conv_4_stride\n",
    "        conv_6_stride = conv_5_stride\n",
    "\n",
    "        log(\"Sride is at \" + str(conv_1_stride))\n",
    "        \n",
    "        for conv_1_kernel in range(3, 6):\n",
    "            conv_2_kernel = conv_1_kernel\n",
    "            conv_3_kernel = conv_2_kernel\n",
    "            conv_4_kernel = conv_3_kernel\n",
    "            conv_5_kernel = conv_4_kernel\n",
    "            conv_6_kernel = conv_5_kernel\n",
    "            \n",
    "            if conv_1_kernel == 4:\n",
    "                conv_1_pad += 1\n",
    "                conv_2_pad += 1\n",
    "                conv_3_pad += 1\n",
    "            \n",
    "            for conv_1_out, conv_2_out, conv_3_out, conv_4_out, conv_5_out, conv_6_out in [(15, 30, 45, 60, 75, 90), (20, 30, 40, 50, 60, 70), (20, 40, 40, 60, 60, 80)]:\n",
    "\n",
    "                cnn = CNN()\n",
    "\n",
    "\n",
    "                cnn.add_layer(nn.Conv2d(1, conv_1_out, conv_1_kernel, padding=conv_1_pad, stride=conv_1_stride))\n",
    "                side_len = int((64 - conv_1_kernel + (2 * conv_1_pad)) / conv_1_stride) + 1\n",
    "\n",
    "                cnn.add_layer(nn.ReLU())\n",
    "                cnn.add_layer(nn.MaxPool2d(2,2))\n",
    "                side_len = int(side_len / 2)\n",
    "\n",
    "#                 print(side_len)\n",
    "                \n",
    "                \n",
    "                cnn.add_layer(nn.BatchNorm2d(conv_1_out))\n",
    "                cnn.add_layer(nn.Conv2d(conv_1_out, conv_2_out, conv_2_kernel, padding=conv_2_pad, stride=conv_2_stride))\n",
    "                side_len = int((side_len - conv_2_kernel + (2 * conv_2_pad)) / conv_2_stride) + 1\n",
    "\n",
    "                cnn.add_layer(nn.ReLU())\n",
    "                if (conv_2_kernel < 5):\n",
    "                    cnn.add_layer(nn.MaxPool2d(2, 2))\n",
    "                    side_len = int(side_len / 2)\n",
    "\n",
    "#                 print(side_len)\n",
    "                \n",
    "                \n",
    "                cnn.add_layer(nn.BatchNorm2d(conv_2_out))\n",
    "                cnn.add_layer(nn.Conv2d(conv_2_out, conv_3_out, conv_3_kernel, padding=conv_3_pad, stride=conv_3_stride))\n",
    "                side_len = int((side_len - conv_3_kernel + (2 * conv_3_pad)) / conv_3_stride) + 1\n",
    "\n",
    "                cnn.add_layer(nn.ReLU())\n",
    "#                 cnn.add_layer(nn.MaxPool2d(2, 2))\n",
    "#                 side_len = int(side_len / 2)\n",
    "\n",
    "#                 print(side_len)\n",
    "\n",
    "    \n",
    "                cnn.add_layer(nn.BatchNorm2d(conv_3_out))\n",
    "                cnn.add_layer(nn.Conv2d(conv_3_out, conv_4_out, conv_4_kernel, padding=conv_4_pad, stride=conv_4_stride))\n",
    "                side_len = int((side_len - conv_4_kernel + (2 * conv_4_pad)) / conv_4_stride) + 1\n",
    "\n",
    "        \n",
    "                cnn.add_layer(nn.ReLU())\n",
    "#                 cnn.add_layer(nn.MaxPool2d(2, 2))\n",
    "#                 side_len = int(side_len / 2)\n",
    "                \n",
    "#                 print(side_len)\n",
    "\n",
    "\n",
    "                cnn.add_layer(nn.BatchNorm2d(conv_4_out))\n",
    "                cnn.add_layer(nn.Conv2d(conv_4_out, conv_5_out, conv_5_kernel, padding=conv_5_pad, stride=conv_5_stride))\n",
    "                side_len = int((side_len - conv_5_kernel + (2 * conv_5_pad)) / conv_5_stride) + 1\n",
    "\n",
    "                cnn.add_layer(nn.ReLU())\n",
    "#                 cnn.add_layer(nn.MaxPool2d(2, 2))\n",
    "#                 side_len = int(side_len / 2)\n",
    "\n",
    "#                 print(side_len)\n",
    "\n",
    "\n",
    "                cnn.add_layer(nn.BatchNorm2d(conv_5_out))\n",
    "                cnn.add_layer(nn.Conv2d(conv_5_out, conv_6_out, conv_6_kernel, padding=conv_6_pad, stride=conv_6_stride))\n",
    "                side_len = int((side_len - conv_6_kernel + (2 * conv_6_pad)) / conv_6_stride) + 1\n",
    "\n",
    "        \n",
    "                cnn.add_layer(nn.ReLU())\n",
    "#                 cnn.add_layer(nn.MaxPool2d(2, 2))\n",
    "#                 side_len = int(side_len / 2)\n",
    "                \n",
    "#                 print(side_len)\n",
    "\n",
    "\n",
    "#                 print(str(conv_6_out * side_len**2))\n",
    "                \n",
    "                cnn.add_layer(View_Layer(-1, conv_6_out * side_len**2))\n",
    "                cnn.add_layer(nn.BatchNorm1d(conv_6_out * side_len**2))\n",
    "                cnn.add_layer(nn.Linear(conv_6_out * side_len**2, 120))\n",
    "                cnn.add_layer(nn.ReLU())\n",
    "                cnn.add_layer(nn.BatchNorm1d(120))\n",
    "                cnn.add_layer(nn.Linear(120, 84))\n",
    "                cnn.add_layer(nn.ReLU())\n",
    "                cnn.add_layer(nn.BatchNorm1d(84))\n",
    "                cnn.add_layer(nn.Linear(84, 10))\n",
    "\n",
    "                cnn.double()\n",
    "                cnn.cuda()\n",
    "\n",
    "                fit(cnn, x_tr, y_tr, 15, lr=0.05)\n",
    "                acc = get_accuracy(cnn, data[\"x_valid\"], data[\"y_valid\"])\n",
    "                log(str(conv_1_out) + \"\\t\" + str(conv_2_out) + \"\\t\" + str(conv_3_out) + \"\\t\" + str(conv_4_out) + \"\\t\" + str(conv_5_out) + \"\\t\" + str(conv_6_out) + \"\\t\" + str(conv_1_kernel))\n",
    "                log(\"Accuracy: \" + str(acc))\n",
    "                log(\"Train Accuracy: \" + str(get_accuracy(cnn, data[\"x_train\"], data[\"y_train\"])))\n",
    "\n",
    "                if (acc > best_acc):\n",
    "                    best_acc = acc\n",
    "                    best_1_out = conv_1_out\n",
    "                    best_2_out = conv_2_out\n",
    "                    best_1_groups = conv_1_kernel\n",
    "                    best_2_groups = conv_2_kernel\n",
    "                    best_1_stride = conv_1_stride\n",
    "                    best_2_stride = conv_2_stride\n",
    "                    best_1_pad = conv_1_pad\n",
    "                    best_2_pad = conv_2_pad\n",
    "\n",
    "                                    \n",
    "log(\"Accuracy:\" + str(best_acc))\n",
    "log(str(best_1_out))\n",
    "log(str(best_2_out))\n",
    "log(str(best_1_kernel))\n",
    "log(str(best_2_kernel))\n",
    "log(str(best_1_stride))\n",
    "log(str(best_2_stride))\n",
    "log(str(best_1_pad))\n",
    "log(str(best_2_pad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-03-19 15:22:49.745505 14\t22\t5\t6\n",
      "2018-03-19 15:22:49.745704 Accuracy: 0.1018\n",
      "Here\n",
      "2018-03-19 15:23:12.781793 14\t18\t5\t6\n",
      "2018-03-19 15:23:12.781987 Accuracy: 0.1321\n",
      "Here\n",
      "2018-03-19 15:23:35.191269 14\t14\t5\t6\n",
      "2018-03-19 15:23:35.191460 Accuracy: 0.1018\n",
      "Here\n",
      "2018-03-19 15:23:57.174216 14\t10\t5\t6\n",
      "2018-03-19 15:23:57.174406 Accuracy: 0.1019\n",
      "Here\n",
      "2018-03-19 15:24:26.556990 18\t22\t5\t6\n",
      "2018-03-19 15:24:26.557188 Accuracy: 0.1314\n",
      "Here\n",
      "2018-03-19 15:24:55.583448 18\t18\t5\t6\n",
      "2018-03-19 15:24:55.583646 Accuracy: 0.1018\n",
      "Here\n",
      "2018-03-19 15:25:23.490491 18\t14\t5\t6\n",
      "2018-03-19 15:25:23.490689 Accuracy: 0.1581\n",
      "Here\n",
      "2018-03-19 15:25:51.036691 18\t10\t5\t6\n",
      "2018-03-19 15:25:51.036880 Accuracy: 0.1018\n",
      "Here\n",
      "2018-03-19 15:26:25.798099 22\t22\t5\t6\n",
      "2018-03-19 15:26:25.798292 Accuracy: 0.1147\n",
      "Here\n",
      "2018-03-19 15:26:59.991511 22\t18\t5\t6\n",
      "2018-03-19 15:26:59.991698 Accuracy: 0.1474\n",
      "Here\n",
      "2018-03-19 15:27:33.009478 22\t14\t5\t6\n",
      "2018-03-19 15:27:33.009681 Accuracy: 0.1018\n",
      "Here\n",
      "2018-03-19 15:28:05.309634 22\t10\t5\t6\n",
      "2018-03-19 15:28:05.309833 Accuracy: 0.1018\n",
      "Here\n",
      "2018-03-19 15:28:05.309996 Accuracy:0.1581\n",
      "2018-03-19 15:28:05.310108 18\n",
      "2018-03-19 15:28:05.310208 14\n",
      "2018-03-19 15:28:05.310303 5\n",
      "2018-03-19 15:28:05.310395 6\n",
      "2018-03-19 15:28:05.310486 1\n",
      "2018-03-19 15:28:05.310576 1\n",
      "2018-03-19 15:28:05.310664 0\n",
      "2018-03-19 15:28:05.310756 0\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch-353)",
   "language": "python",
   "name": "pytorch-353"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
